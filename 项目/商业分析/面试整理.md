# TOT

Q_spaceä½¿ç”¨json/yamlæ ¼å¼æž„å»ºä¸€ä¸ªåŠ¨æ€çš„æ ‘çŠ¶é—®é¢˜æ¨¡åž‹ï¼Œæ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªæ€è€ƒç‚¹æˆ–è€…è‡ªé—®é¢˜ï¼Œå›¾è°±å¼æŽ¨ç†ã€‚ä¸æ˜¯ç®€å•çš„çº¿æ€§ä»»åŠ¡ï¼Œå¯ä»¥åœ¨æ ‘çš„ä¸åŒçš„åˆ†æ”¯é—´æ¡çº¦çš„

åŠ¨æ€æ¼”åŒ–ï¼šæ ‘ç»“æž„ä¼šæ ¹æ®æ–°çš„å‘çŽ°çš„ä¿¡æ¯åŠ¨æ€æ‰©å±•å’Œè°ƒæ•´

ä¸»é—®é¢˜ï¼Œå­é—®é¢˜ã€‚å‡è®¾ã€‚æ¦‚å¿µå®šä¹‰ï¼Œæ½œåœ¨å› ç´ ï¼Œæ•°æ®ç‚¹åˆ†ç±»ï¼Œ

å¤šagentåä½œçš„totæŽ¢ç´¢

å¤šä¸ªagentåä½œæ¥æŽ¢ç´¢TPTçš„ä¸åŒåˆ†æ”¯ï¼ŒManagerAgentä½œä¸ºTPTçš„å¯¼èˆªå‘˜ã€‚è§£æžå½“å‰çš„Qspaceçš„è§£é›‡ï¼Œè¯†åˆ«å½“å‰éœ€è¦æŽ¢ç´¢çš„èŠ‚ç‚¹ï¼Œè®²æŠ½è±¡é—®é¢˜è½¬æ¢æˆä¸ºå…·ä½“çš„ä»»åŠ¡ï¼ŒåŠ¨æ€æ‰©å±•é—®é¢˜æ ‘

æ‰§è¡Œagentï¼Œè´Ÿè´£å…·ä½“çš„æŽ¢ç´¢tptçš„å¶å­èŠ‚ç‚¹

information gather:æœç´¢ä¿¡æ¯

strategicã€‚analyst:åˆ†æžæŽ¨ç†

çŸ¥è¯†çŠ¶æ€å’Œæ€è€ƒæ ‘çš„åŒå‘åé¦ˆ

wisebaseä½œä¸ºçŸ¥è¯†åº“

q_spaceæ€è€ƒæ ‘ï¼Œç”Ÿæˆä»»åŠ¡ï¼Œagentæ‰§è¡Œï¼Œæ›´æ–°qspaceçš„çŠ¶æ€ï¼Œå‘çŽ°æ–°çš„é—®é¢˜ã€‚æ‰©å±•æ€è€ƒæ ‘

totçš„åŠ¨æ€è°ƒæ•´æœºåˆ¶ï¼Œmanager agentä¼šæŒç»­ï¼Œè¯»å–å½“å‰çš„æ€è€ƒæ ‘çš„çŠ¶æ€ï¼Œè¯†åˆ«å¯è¡ŒåŠ¨çš„èŠ‚ç‚¹ã€‚åŸºäºŽæ–°çš„ä¿¡æ¯è°ƒæ•´æ ‘çš„ç»“æž„ï¼Œå¤„ç†èŠ‚ç‚¹é—´çš„å†²çªã€‚æ‰©å±•æ–°çš„æ€è€ƒåˆ†æ”¯


å¤šéƒ¨é—¨çš„å¹¶è¡Œçš„totçš„æŽ¢ç´¢

é¡¹ç›®æ”¯æŒå¤šä¸ªéƒ¨é—¨çš„crewçš„åŒæ—¶è¿è¡Œå„è‡ªçš„totçš„

ç«žäº‰åˆ†æžéƒ¨é—¨ï¼šæž„å»ºç«žäº‰æ ¼å±€çš„æ€è€ƒæ ‘

ç”¨æˆ·ç ”ç©¶éƒ¨é—¨ï¼šæž„å»ºç”¨æˆ·éœ€æ±‚çš„æ€è€ƒæ ‘

åœºæ™¯æŒ–æŽ˜éƒ¨é—¨ï¼šæž„å»ºåº”ç”¨åœºæ™¯çš„æ€è€ƒæ ‘

å„ä¸ªéƒ¨é—¨çš„æ€è€ƒæ ‘é€šè¿‡å…±äº«çš„wisebaseè¿›è¡Œä¿¡æ¯çš„äº¤æ¢çš„

ç»“æž„åŒ–æ€ç»´ï¼ŒåŠ¨æ€æ¼”åŒ–ï¼Œå¤šè§’åº¦æŽ¢ç´¢ï¼Œå¤šä¸ªagentå¹¶è¡ŒæŽ¢ç´¢ä¸åŒçš„åˆ†æ”¯ï¼ŒçŠ¶æ€ç®¡ç†ï¼Œæ¯ä¸ªæ€è€ƒèŠ‚ç‚¹éƒ½æœ‰æ˜Žç¡®çš„çŠ¶æ€ï¼ŒçŸ¥è¯†æ•´åˆï¼Œé€šè¿‡wisebaseå®žçŽ°æ€è€ƒç»“æž„çš„ç»“æž„åŒ–çš„å­˜å‚¨ï¼Œåä½œæŽ¨ç†ï¼Œå¤šä¸ªéƒ¨é—¨çš„æ€è€ƒæ ‘é¦™æ¹–åä½œçš„

åˆ†è§£-ã€‹æŽ¢ç´¢-ã€‰æ•´åˆ


è¯­ä¹‰æ˜¯å®¹æ˜“æ··æ·†çš„ï¼ŒåŒæ ·çš„ä¸€ä¸ªè§‚ç‚¹ï¼Œåœ¨ä¸åŒçš„é—®é¢˜ä¸Žä¸‹çš„é¢„è­¦ä¸‹å¯èƒ½æœ‰ä¸åŒçš„å«ä¹‰ï¼Œå¤šä¸€å­—å°‘ä¸€å­—éƒ½ä¼šé€ æˆä¿¡æ¯çš„åå·®ï¼Œä¸å¸Œæœ›å¹»è§‰è¢«å±‚å±‚æ”¾å¤§


wisebaseä½œä¸ºä¿¡æ¯ç¼“å†²æœŸï¼Œç»“åˆæ˜Žç¡®çš„ä¿¡æ¯åˆ†ç±»å’ŒåŒæ­¥æœºåˆ¶ï¼Œæœ‰åŠ©äºŽé¿å…æœªç»æ¸…æ´—çš„å¯“æ„ç›´æŽ¥å½±å“éƒ¨é—¨å†…å®¹çš„è¿ä½œ

ä¿¡æ¯å’Œç»“è®ºéœ€è¦åœ¨ä¸åŒçš„éƒ¨é—¨æµé€š

æ¯ä¸ªéƒ¨é—¨éƒ½æœ‰è‡ªå·±çš„wisebaseï¼Œè·¨éƒ¨é—¨äº¤äº’å®žé™…ä¸Šæ˜¯è¿™äº›çŸ¥è¯†çŠ¶æ€å­˜å‚¨ï¼Œåœ¨é¦™æ¹–æ²Ÿé€šçš„

é€šè¿‡è·¨éƒ¨é—¨çš„ä¿¡æ¯åŒæ­¥åŠŸèƒ½ï¼Œä¸€ä¸ªéƒ¨é—¨å¯ä»¥æŸ¥è¯¢å…¶ä»–çš„éƒ¨é—¨çš„wisebaseå¹¶ä¸”ä»Žä¸­æå–å¯¹è‡ªå·±çš„éƒ¨é—¨é—®é¢˜ä¸Žæœ‰å¸®åŠ©çš„ä¿¡æ¯ï¼Œ

é€šè¿‡ **è·¨éƒ¨é—¨ä¿¡æ¯åŒæ­¥** åŠŸèƒ½ï¼Œä¸€ä¸ªéƒ¨é—¨å¯ä»¥æŸ¥è¯¢å…¶ä»–éƒ¨é—¨çš„ `Wisebase`ï¼Œå¹¶ä»Žä¸­æå–å¯¹è‡ªå·±éƒ¨é—¨é—®é¢˜åŸŸæœ‰å¸®åŠ©çš„ä¿¡æ¯ï¼ˆåŒæ ·ä¿ç•™factsã€pointsç­‰ç±»åˆ«ï¼‰ã€‚
ç›¸åº”çš„ **ä»»åŠ¡è§„åˆ’ä¸Žç”Ÿæˆ** åŠŸèƒ½ä¼šè´Ÿè´£æ ¹æ® `Wisebase` çš„å˜åŒ–ï¼ˆåŒ…æ‹¬ä»Žå…¶ä»–éƒ¨é—¨åŒæ­¥æ¥çš„ä¿¡æ¯ï¼‰ï¼Œæ¥è°ƒæ•´ `Q_space` ä¸­çš„ä»»åŠ¡ã€‚

å·¥ä½œæµç¨‹ï¼š

ç”¨æˆ·è¾“å…¥brief,æœ‰å¤šä¸ªéƒ¨é—¨ï¼Œä¾‹å¦‚ç«žäº‰åˆ†æžéƒ¨é—¨ï¼Œç”¨æˆ·ä½“éªŒç ”ç©¶éƒ¨é—¨ï¼Œæ¯ä¸ªéƒ¨é—¨åŒ…æ‹¬åŠŸèƒ½å±‚å’Œæ‰§è¡Œå±‚ï¼Œæ¯ä¸ªéƒ½æœ‰ä¸€ç³»åˆ—çš„agentæˆ–è€…æ¨¡å—ç»„ä»¶ï¼ŒwiasebaseçŸ¥è¯†çŠ¶æ€å’Œq_spaceä»»åŠ¡çŠ¶æ€é€šè¿‡æ¡ä»¶è§¦å‘å™¨æ¥å‡ºå‘æ‰§è¡Œçš„

è¾“å…¥å¤„ç†å’Œç»“æž„åŒ–çš„ï¼šå‡ºå‘æ¡ä»¶ï¼šæŽ¥æ”¶åˆ°ç”¨æˆ·è¾“å…¥çš„briefæˆ–è€…å¤–éƒ¨ä¿¡æ¯

åŠŸèƒ½ï¼šè§£æžå‡ºå‘ï¼Œæå–æ„å›¾å’Œå…³é”®ä¿¡æ¯è¿›è¡Œå¯“æ„çš„æ¸…æ™°ï¼Œ

è®²ç»“æž„åŒ–ä¿¡æ¯æ·»åŠ ä¸ºwiasebaseçš„åŽ¨å¸ˆæ¡ç›®

çŸ¥è¯†çŠ¶æ€çš„ç®¡ç†

æŽ¥æ”¶åˆ°æ¥è‡ªæ‰§è¡Œå±‚çš„ç»“æžœæˆ–è€…æ–°çš„ä¿¡æ¯çš„è¾“å…¥

å¯¹ä¿¡æ¯è¿›è¡Œåˆ†ç±»factï¼Œhypothesisï¼ŒpointséªŒè¯æ›´æ–°wiasebaseçš„å†…å®¹çš„

çŠ¶æ€å½±å“ï¼šç»´æŠ¤wiasebaseçš„åŠ¨æ€å˜åŒ–ï¼Œååº”å½“å‰çš„è®¤çŸ¥çš„çŠ¶æ€çš„


ä»»åŠ¡è§„åˆ’çš„ç”Ÿæˆ

å‡ºå‘æ¡ä»¶wiaseçš„çŠ¶æ€å‘ç”Ÿæ˜¾è‘—å˜åŒ–æˆ–è€…å¤§é“çš„è§„åˆ’çš„é¢„çŸ¥

åŸºäºŽwiaseçš„ä¿¡æ¯ï¼Œç”Ÿæˆæˆ–è€…å†¬ç“œæ±¤çš„é¢˜æ€»é¢å¥½éš¾è¿‡qpaseæ˜Žç¡®ä»»åŠ¡å±‚çº§å†…å®¹è¾“å‡ºå®šä¹‰å’Œæ‰§è¡Œè€…çš„åˆ†é…çš„ï¼Œ


wisebase

ç”¨æˆ·è¾“å…¥briefåŽç®¡ç†åŠŸèƒ½çš„è¾“å…¥å¤„ç†å’Œç»“æž„åŒ–æ¨¡å—ä¼šæ ¹æ®briefæ‹†è§£ç»†è…»æž„å»ºå‡ºäº‹çš„wisebaseï¼Œéœ€è¦è€ƒè™‘ç”¨æˆ·æ„å›¾éƒ¨é—¨çš„æ™ºèƒ½æ‰§è¡Œè€…çš„èƒ½åŠ›ç­‰ç­‰

æ¯ä¸ªéƒ¨é—¨éƒ½æœ‰è‡ªå·±çš„wiasebaseä½œä¸ºæ ¸å¿ƒçš„çŸ¥è¯†çŠ¶æ€çš„å­˜å‚¨ï¼Œå¹¶ä¸”æ˜¯åŠ¨æ€å˜åŒ–çš„ï¼Œ

wiasebaseçš„ç±»åž‹ï¼Œfactså¼æ˜¯ç´¯çš„ä¿¡æ¯ç¡®å‡¿çš„

hypotheissiå‡è®¾éœ€è¦æ±‚è¯çš„

pointsè§‚ç‚¹å’Œæ´žè§ï¼Œç­æ³•å®žé™…è¯æ˜Žçš„ä½†æ˜¯make sense

é—®é¢˜ä½™çš„ï¼š

æ ¹æ®wisebaseçš„çŠ¶æ€ï¼Œç®¡ç†åŠŸèƒ½çš„ä»»åŠ¡è§„åˆ’å’Œç”Ÿæˆçš„ï¼Œæ¨¡å—ä¼šç”Ÿæˆå¹¶ä¸”åŠ¨æ€ç»´æŠ¤ä¸€ä¸ªé—®é¢˜ä½™ä½œä¸ºæ ¸å¿ƒçš„ä»»åŠ¡çŠ¶æ€çš„ï¼Œè®²æ˜Žç¡®é—®é¢˜çš„è¦ç´ å–é…’ç§‘ä¸¾è·¯å¾„ç›¸å½“äºŽé—®é¢˜å»ºæ¨¡

é—®é¢˜å»ºæ¨¡çš„æ„ä¹‰åœ¨äºŽå…ˆæžæ¸…æ¥šæœ¬è´¨æ˜¯ä»€ä¹ˆï¼Œéœ€è¦ä¸€æ­¥æ­¥å¼„æ¸…æ¥šçš„å½±å“å› ç´ æœ‰å“ªäº›ï¼Œå°±è¡Œæ±‚è§£æ•°å­¦é—®é¢˜ä¸€æ ·çš„


å¯åŠ¨-é…ç½®åŠ è½½-breifåˆ†æžï¼Œcrewé€‰æ‹©ï¼Œåˆå§‹åŒ–ï¼Œæ‰§è¡Œï¼Œç»“æžœæ•´åˆçš„

é»˜è®¤å¯åŠ¨ç«žäº‰åˆ†æžéƒ¨é—¨ï¼Œå¦‚æžœbrefå½“ä¸­åŒ…å«ç”¨æˆ·æˆ–è€…éœ€æ±‚å…³é”®è¯éœ€è¦é¢å¤–å¯åŠ¨user_researchç”¨æˆ·ç ”ç©¶éƒ¨é—¨

åˆå§‹åŒ–qpaseï¼Œåˆ›å»ºwiasebaseçš„å·¥å…·ï¼Œåˆå§‹åŒ–agentsï¼Œ
ä»»åŠ¡åˆ†è§£ï¼Œmanageå°†é—®é¢˜æ ‘èŠ‚ç‚¹è½¬åŒ–æˆä¸ºå…·ä½“çš„ä»»åŠ¡ï¼Œagentæ‰§è¡Œä¿¡æ¯æœé›†å…ƒå’Œç­–ç•¥åˆ†æžå¸ˆæ‰§è¡Œå…·ä½“çš„ä»»åŠ¡ï¼ŒçŸ¥è¯†æ›´æ–°ï¼Œæ‰§è¡Œç»“æžœå†™å…¥wiasebaseï¼Œè¿­ä»£æ›´æ–°ï¼Œæ ¹æ®æ–°çš„çŸ¥è¯†è°ƒæ•´qpaseï¼Œç”Ÿæˆæ–°çš„ä»»åŠ¡

ç”¨æˆ·brief->dispatcher->crewé€‰æ‹©-ã€‹åˆå§‹åŒ–

manager agent-ã€‰qpaseæž„å»º-ã€‹ä»»åŠ¡åˆ†è§£

æ‰§è¡Œagents-ã€‰å·¥å…·è°ƒç”¨0ã€‹wiasebaseæ›´æ–°

çŠ¶æ€åé¦ˆ-ã€‚qpaseè°ƒæ•´ï¼Œæ–°ä»»åŠ¡ç”Ÿæˆ

crewq,agents,tasks,prcoess,manager,vernoseå±‚çº§åŒ–çš„ä»»åŠ¡åˆ†é…ï¼Œä»»åŠ¡æ‰§è¡Œå’ŒçŠ¶æ€ç®¡ç†ï¼Œagenté—´çš„é€šä¿¡å’Œåè°ƒ

pydanticæ•°æ®éªŒè¯ï¼Œnest-asynciioå¼‚æ­¥æ”¯æŒï¼Œ

node+flow+shaored store 

node+flow+sthared store

å…±äº«å­˜å‚¨çš„æ¨¡å¼

pocketflow
q-spaceé—®é¢˜çŠ¶æ€çš„åŠŸæ•ˆå­˜å‚¨ï¼Œ

wiasebaseï¼šè·¨aegntçš„çŸ¥è¯†å…±äº«å­˜å‚¨

---
description: pocketflow
globs: 
alwaysApply: false
---
---
layout: default
title: "Agentic Coding"
---

# Agentic Coding: Humans Design, Agents code!

> If you are an AI agents involved in building LLM Systems, read this guide **VERY, VERY** carefully! This is the most important chapter in the entire document. Throughout development, you should always (1) start with a small and simple solution, (2) design at a high level (`docs/design.md`) before implementation, and (3) frequently ask humans for feedback and clarification.
{: .warning }

## Agentic Coding Steps

Agentic Coding should be a collaboration between Human System Design and Agent Implementation:

| Steps                  | Human      | AI        | Comment                                                                 |
|:-----------------------|:----------:|:---------:|:------------------------------------------------------------------------|
| 1. Requirements | â˜…â˜…â˜… High  | â˜…â˜†â˜† Low   | Humans understand the requirements and context.                    |
| 2. Flow          | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium |  Humans specify the high-level design, and the AI fills in the details. |
| 3. Utilities   | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans provide available external APIs and integrations, and the AI helps with implementation. |
| 4. Node          | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  | The AI helps design the node types and data handling based on the flow.          |
| 5. Implementation      | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI implements the flow based on the design. |
| 6. Optimization        | â˜…â˜…â˜† Medium | â˜…â˜…â˜† Medium | Humans evaluate the results, and the AI helps optimize. |
| 7. Reliability         | â˜…â˜†â˜† Low   | â˜…â˜…â˜… High  |  The AI writes test cases and addresses corner cases.     |

1. **Requirements**: Clarify the requirements for your project, and evaluate whether an AI system is a good fit. 
    - Understand AI systems' strengths and limitations:
      - **Good for**: Routine tasks requiring common sense (filling forms, replying to emails)
      - **Good for**: Creative tasks with well-defined inputs (building slides, writing SQL)
      - **Not good for**: Ambiguous problems requiring complex decision-making (business strategy, startup planning)
    - **Keep It User-Centric:** Explain the "problem" from the user's perspective rather than just listing features.
    - **Balance complexity vs. impact**: Aim to deliver the highest value features with minimal complexity early.

2. **Flow Design**: Outline at a high level, describe how your AI system orchestrates nodes.
    - Identify applicable design patterns (e.g., [Map Reduce](mdc:design_pattern/mapreduce.md), [Agent](mdc:design_pattern/agent.md), [RAG](mdc:design_pattern/rag.md)).
      - For each node in the flow, start with a high-level one-line description of what it does.
      - If using **Map Reduce**, specify how to map (what to split) and how to reduce (how to combine).
      - If using **Agent**, specify what are the inputs (context) and what are the possible actions.
      - If using **RAG**, specify what to embed, noting that there's usually both offline (indexing) and online (retrieval) workflows.
    - Outline the flow and draw it in a mermaid diagram. For example:
      ```mermaid
      flowchart LR
          start[Start] --> batch[Batch]
          batch --> check[Check]
          check -->|OK| process
          check -->|Error| fix[Fix]
          fix --> check
          
          subgraph process[Process]
            step1[Step 1] --> step2[Step 2]
          end
          
          process --> endNode[End]
      ```
    - > **If Humans can't specify the flow, AI Agents can't automate it!** Before building an LLM system, thoroughly understand the problem and potential solution by manually solving example inputs to develop intuition.  
      {: .best-practice }

3. **Utilities**: Based on the Flow Design, identify and implement necessary utility functions.
    - Think of your AI system as the brain. It needs a bodyâ€”these *external utility functions*â€”to interact with the real world:
        <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

        - Reading inputs (e.g., retrieving Slack messages, reading emails)
        - Writing outputs (e.g., generating reports, sending emails)
        - Using external tools (e.g., calling LLMs, searching the web)
        - **NOTE**: *LLM-based tasks* (e.g., summarizing text, analyzing sentiment) are **NOT** utility functions; rather, they are *core functions* internal in the AI system.
    - For each utility function, implement it and write a simple test.
    - Document their input/output, as well as why they are necessary. For example:
      - `name`: `get_embedding` (`utils/get_embedding.py`)
      - `input`: `str`
      - `output`: a vector of 3072 floats
      - `necessity`: Used by the second node to embed text
    - Example utility implementation:
      ```python
      # utils/call_llm.py
      from openai import OpenAI

      def call_llm(prompt):    
          client = OpenAI(api_key="YOUR_API_KEY_HERE")
          r = client.chat.completions.create(
              model="gpt-4o",
              messages=[{"role": "user", "content": prompt}]
          )
          return r.choices[0].message.content
          
      if __name__ == "__main__":
          prompt = "What is the meaning of life?"
          print(call_llm(prompt))
      ```
    - > **Sometimes, design Utilies before Flow:**  For example, for an LLM project to automate a legacy system, the bottleneck will likely be the available interface to that system. Start by designing the hardest utilities for interfacing, and then build the flow around them.
      {: .best-practice }

4. **Node Design**: Plan how each node will read and write data, and use utility functions.
   - One core design principle for PocketFlow is to use a [shared store](mdc:core_abstraction/communication.md), so start with a shared store design:
      - For simple systems, use an in-memory dictionary.
      - For more complex systems or when persistence is required, use a database.
      - **Don't Repeat Yourself**: Use in-memory references or foreign keys.
      - Example shared store design:
        ```python
        shared = {
            "user": {
                "id": "user123",
                "context": {                # Another nested dict
                    "weather": {"temp": 72, "condition": "sunny"},
                    "location": "San Francisco"
                }
            },
            "results": {}                   # Empty dict to store outputs
        }
        ```
   - For each [Node](mdc:core_abstraction/node.md), describe its type, how it reads and writes data, and which utility function it uses. Keep it specific but high-level without codes. For example:
     - `type`: Regular (or Batch, or Async)
     - `prep`: Read "text" from the shared store
     - `exec`: Call the embedding utility function
     - `post`: Write "embedding" to the shared store

5. **Implementation**: Implement the initial nodes and flows based on the design.
   - ðŸŽ‰ If you've reached this step, humans have finished the design. Now *Agentic Coding* begins!
   - **"Keep it simple, stupid!"** Avoid complex features and full-scale type checking.
   - **FAIL FAST**! Avoid `try` logic so you can quickly identify any weak points in the system.
   - Add logging throughout the code to facilitate debugging.

7. **Optimization**:
   - **Use Intuition**: For a quick initial evaluation, human intuition is often a good start.
   - **Redesign Flow (Back to Step 3)**: Consider breaking down tasks further, introducing agentic decisions, or better managing input contexts.
   - If your flow design is already solid, move on to micro-optimizations:
     - **Prompt Engineering**: Use clear, specific instructions with examples to reduce ambiguity.
     - **In-Context Learning**: Provide robust examples for tasks that are difficult to specify with instructions alone.

   - > **You'll likely iterate a lot!** Expect to repeat Steps 3â€“6 hundreds of times.
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     {: .best-practice }

8. **Reliability**  
   - **Node Retries**: Add checks in the node `exec` to ensure outputs meet requirements, and consider increasing `max_retries` and `wait` times.
   - **Logging and Visualization**: Maintain logs of all attempts and visualize node results for easier debugging.
   - **Self-Evaluation**: Add a separate node (powered by an LLM) to review outputs when results are uncertain.

## Example LLM Project File Structure

```
my_project/
â”œâ”€â”€ main.py
â”œâ”€â”€ nodes.py
â”œâ”€â”€ flow.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ call_llm.py
â”‚   â””â”€â”€ search_web.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs/
    â””â”€â”€ design.md
```

- **`docs/design.md`**: Contains project documentation for each step above. This should be *high-level* and *no-code*.
- **`utils/`**: Contains all utility functions.
  - It's recommended to dedicate one Python file to each API call, for example `call_llm.py` or `search_web.py`.
  - Each file should also include a `main()` function to try that API call
- **`nodes.py`**: Contains all the node definitions.
  ```python
  # nodes.py
  from pocketflow import Node
  from utils.call_llm import call_llm

  class GetQuestionNode(Node):
      def exec(self, _):
          # Get question directly from user input
          user_question = input("Enter your question: ")
          return user_question
      
      def post(self, shared, prep_res, exec_res):
          # Store the user's question
          shared["question"] = exec_res
          return "default"  # Go to the next node

  class AnswerNode(Node):
      def prep(self, shared):
          # Read question from shared
          return shared["question"]
      
      def exec(self, question):
          # Call LLM to get the answer
          return call_llm(question)
      
      def post(self, shared, prep_res, exec_res):
          # Store the answer in shared
          shared["answer"] = exec_res
  ```
- **`flow.py`**: Implements functions that create flows by importing node definitions and connecting them.
  ```python
  # flow.py
  from pocketflow import Flow
  from nodes import GetQuestionNode, AnswerNode

  def create_qa_flow():
      """Create and return a question-answering flow."""
      # Create nodes
      get_question_node = GetQuestionNode()
      answer_node = AnswerNode()
      
      # Connect nodes in sequence
      get_question_node >> answer_node
      
      # Create flow starting with input node
      return Flow(start=get_question_node)
  ```
- **`main.py`**: Serves as the project's entry point.
  ```python
  # main.py
  from flow import create_qa_flow

  # Example main function
  # Please replace this with your own main function
  def main():
      shared = {
          "question": None,  # Will be populated by GetQuestionNode from user input
          "answer": None     # Will be populated by AnswerNode
      }

      # Create the flow and run it
      qa_flow = create_qa_flow()
      qa_flow.run(shared)
      print(f"Question: {shared['question']}")
      print(f"Answer: {shared['answer']}")

  if __name__ == "__main__":
      main()
  ```

================================================
File: docs/index.md
================================================
---
layout: default
title: "Home"
nav_order: 1
---

# Pocket Flow

A [100-line](mdc:https:/github.com/the-pocket/PocketFlow/blob/main/pocketflow/__init__.py) minimalist LLM framework for *Agents, Task Decomposition, RAG, etc*.

- **Lightweight**: Just the core graph abstraction in 100 lines. ZERO dependencies, and vendor lock-in.
- **Expressive**: Everything you love from larger frameworksâ€”([Multi-](mdc:design_pattern/multi_agent.html))[Agents](mdc:design_pattern/agent.html), [Workflow](mdc:design_pattern/workflow.html), [RAG](mdc:design_pattern/rag.html), and more.  
- **Agentic-Coding**: Intuitive enough for AI agents to help humans build complex LLM applications.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## Core Abstraction

We model the LLM workflow as a **Graph + Shared Store**:

- [Node](mdc:core_abstraction/node.md) handles simple (LLM) tasks.
- [Flow](mdc:core_abstraction/flow.md) connects nodes through **Actions** (labeled edges).
- [Shared Store](mdc:core_abstraction/communication.md) enables communication between nodes within flows.
- [Batch](mdc:core_abstraction/batch.md) nodes/flows allow for data-intensive tasks.
- [Async](mdc:core_abstraction/async.md) nodes/flows allow waiting for asynchronous tasks.
- [(Advanced) Parallel](mdc:core_abstraction/parallel.md) nodes/flows handle I/O-bound tasks.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="500"/>
</div>

## Design Pattern

From there, itâ€™s easy to implement popular design patterns:

- [Agent](mdc:design_pattern/agent.md) autonomously makes decisions.
- [Workflow](mdc:design_pattern/workflow.md) chains multiple tasks into pipelines.
- [RAG](mdc:design_pattern/rag.md) integrates data retrieval with generation.
- [Map Reduce](mdc:design_pattern/mapreduce.md) splits data tasks into Map and Reduce steps.
- [Structured Output](mdc:design_pattern/structure.md) formats outputs consistently.
- [(Advanced) Multi-Agents](mdc:design_pattern/multi_agent.md) coordinate multiple agents.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="500"/>
</div>

## Utility Function

We **do not** provide built-in utilities. Instead, we offer *examples*â€”please *implement your own*:

- [LLM Wrapper](mdc:utility_function/llm.md)
- [Viz and Debug](mdc:utility_function/viz.md)
- [Web Search](mdc:utility_function/websearch.md)
- [Chunking](mdc:utility_function/chunking.md)
- [Embedding](mdc:utility_function/embedding.md)
- [Vector Databases](mdc:utility_function/vector.md)
- [Text-to-Speech](mdc:utility_function/text_to_speech.md)

**Why not built-in?**: I believe it's a *bad practice* for vendor-specific APIs in a general framework:
- *API Volatility*: Frequent changes lead to heavy maintenance for hardcoded APIs.
- *Flexibility*: You may want to switch vendors, use fine-tuned models, or run them locally.
- *Optimizations*: Prompt caching, batching, and streaming are easier without vendor lock-in.

## Ready to build your Apps? 

Check out [Agentic Coding Guidance](mdc:guide.md), the fastest way to develop LLM projects with Pocket Flow!

================================================
File: docs/core_abstraction/async.md
================================================
---
layout: default
title: "(Advanced) Async"
parent: "Core Abstraction"
nav_order: 5
---

# (Advanced) Async

**Async** Nodes implement `prep_async()`, `exec_async()`, `exec_fallback_async()`, and/or `post_async()`. This is useful for:

1. **prep_async()**: For *fetching/reading data (files, APIs, DB)* in an I/O-friendly way.
2. **exec_async()**: Typically used for async LLM calls.
3. **post_async()**: For *awaiting user feedback*, *coordinating across multi-agents* or any additional async steps after `exec_async()`.

**Note**: `AsyncNode` must be wrapped in `AsyncFlow`. `AsyncFlow` can also include regular (sync) nodes.

### Example

```python
class SummarizeThenVerify(AsyncNode):
    async def prep_async(self, shared):
        # Example: read a file asynchronously
        doc_text = await read_file_async(shared["doc_path"])
        return doc_text

    async def exec_async(self, prep_res):
        # Example: async LLM call
        summary = await call_llm_async(f"Summarize: {prep_res}")
        return summary

    async def post_async(self, shared, prep_res, exec_res):
        # Example: wait for user feedback
        decision = await gather_user_feedback(exec_res)
        if decision == "approve":
            shared["summary"] = exec_res
            return "approve"
        return "deny"

summarize_node = SummarizeThenVerify()
final_node = Finalize()

# Define transitions
summarize_node - "approve" >> final_node
summarize_node - "deny"    >> summarize_node  # retry

flow = AsyncFlow(start=summarize_node)

async def main():
    shared = {"doc_path": "document.txt"}
    await flow.run_async(shared)
    print("Final Summary:", shared.get("summary"))

asyncio.run(main())
```

================================================
File: docs/core_abstraction/batch.md
================================================
---
layout: default
title: "Batch"
parent: "Core Abstraction"
nav_order: 4
---

# Batch

**Batch** makes it easier to handle large inputs in one Node or **rerun** a Flow multiple times. Example use cases:
- **Chunk-based** processing (e.g., splitting large texts).
- **Iterative** processing over lists of input items (e.g., user queries, files, URLs).

## 1. BatchNode

A **BatchNode** extends `Node` but changes `prep()` and `exec()`:

- **`prep(shared)`**: returns an **iterable** (e.g., list, generator).
- **`exec(item)`**: called **once** per item in that iterable.
- **`post(shared, prep_res, exec_res_list)`**: after all items are processed, receives a **list** of results (`exec_res_list`) and returns an **Action**.


### Example: Summarize a Large File

```python
class MapSummaries(BatchNode):
    def prep(self, shared):
        # Suppose we have a big file; chunk it
        content = shared["data"]
        chunk_size = 10000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        return chunks

    def exec(self, chunk):
        prompt = f"Summarize this chunk in 10 words: {chunk}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res_list):
        combined = "\n".join(exec_res_list)
        shared["summary"] = combined
        return "default"

map_summaries = MapSummaries()
flow = Flow(start=map_summaries)
flow.run(shared)
```

---

## 2. BatchFlow

A **BatchFlow** runs a **Flow** multiple times, each time with different `params`. Think of it as a loop that replays the Flow for each parameter set.

### Example: Summarize Many Files

```python
class SummarizeAllFiles(BatchFlow):
    def prep(self, shared):
        # Return a list of param dicts (one per file)
        filenames = list(shared["data"].keys())  # e.g., ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

# Suppose we have a per-file Flow (e.g., load_file >> summarize >> reduce):
summarize_file = SummarizeFile(start=load_file)

# Wrap that flow into a BatchFlow:
summarize_all_files = SummarizeAllFiles(start=summarize_file)
summarize_all_files.run(shared)
```

### Under the Hood
1. `prep(shared)` returns a list of param dictsâ€”e.g., `[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`.
2. The **BatchFlow** loops through each dict. For each one:
   - It merges the dict with the BatchFlowâ€™s own `params`.
   - It calls `flow.run(shared)` using the merged result.
3. This means the sub-Flow is run **repeatedly**, once for every param dict.

---

## 3. Nested or Multi-Level Batches

You can nest a **BatchFlow** in another **BatchFlow**. For instance:
- **Outer** batch: returns a list of diretory param dicts (e.g., `{"directory": "/pathA"}`, `{"directory": "/pathB"}`, ...).
- **Inner** batch: returning a list of per-file param dicts.

At each level, **BatchFlow** merges its own param dict with the parentâ€™s. By the time you reach the **innermost** node, the final `params` is the merged result of **all** parents in the chain. This way, a nested structure can keep track of the entire context (e.g., directory + file name) at once.

```python

class FileBatchFlow(BatchFlow):
    def prep(self, shared):
        directory = self.params["directory"]
        # e.g., files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(BatchFlow):
    def prep(self, shared):
        directories = [ "/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# MapSummaries have params like {"directory": "/path/to/dirA", "filename": "file1.txt"}
inner_flow = FileBatchFlow(start=MapSummaries())
outer_flow = DirectoryBatchFlow(start=inner_flow)
```

================================================
File: docs/core_abstraction/communication.md
================================================
---
layout: default
title: "Communication"
parent: "Core Abstraction"
nav_order: 3
---

# Communication

Nodes and Flows **communicate** in 2 ways:

1. **Shared Store (for almost all the cases)** 

   - A global data structure (often an in-mem dict) that all nodes can read ( `prep()`) and write (`post()`).  
   - Great for data results, large content, or anything multiple nodes need.
   - You shall design the data structure and populate it ahead.
     
   - > **Separation of Concerns:** Use `Shared Store` for almost all cases to separate *Data Schema* from *Compute Logic*!  This approach is both flexible and easy to manage, resulting in more maintainable code. `Params` is more a syntax sugar for [Batch](mdc:batch.md).
     {: .best-practice }

2. **Params (only for [Batch](mdc:batch.md))** 
   - Each node has a local, ephemeral `params` dict passed in by the **parent Flow**, used as an identifier for tasks. Parameter keys and values shall be **immutable**.
   - Good for identifiers like filenames or numeric IDs, in Batch mode.

If you know memory management, think of the **Shared Store** like a **heap** (shared by all function calls), and **Params** like a **stack** (assigned by the caller).

---

## 1. Shared Store

### Overview

A shared store is typically an in-mem dictionary, like:
```python
shared = {"data": {}, "summary": {}, "config": {...}, ...}
```

It can also contain local file handlers, DB connections, or a combination for persistence. We recommend deciding the data structure or DB schema first based on your app requirements.

### Example

```python
class LoadData(Node):
    def post(self, shared, prep_res, exec_res):
        # We write data to shared store
        shared["data"] = "Some text content"
        return None

class Summarize(Node):
    def prep(self, shared):
        # We read data from shared store
        return shared["data"]

    def exec(self, prep_res):
        # Call LLM to summarize
        prompt = f"Summarize: {prep_res}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res):
        # We write summary to shared store
        shared["summary"] = exec_res
        return "default"

load_data = LoadData()
summarize = Summarize()
load_data >> summarize
flow = Flow(start=load_data)

shared = {}
flow.run(shared)
```

Here:
- `LoadData` writes to `shared["data"]`.
- `Summarize` reads from `shared["data"]`, summarizes, and writes to `shared["summary"]`.

---

## 2. Params

**Params** let you store *per-Node* or *per-Flow* config that doesn't need to live in the shared store. They are:
- **Immutable** during a Node's run cycle (i.e., they don't change mid-`prep->exec->post`).
- **Set** via `set_params()`.
- **Cleared** and updated each time a parent Flow calls it.

> Only set the uppermost Flow params because others will be overwritten by the parent Flow. 
> 
> If you need to set child node params, see [Batch](mdc:batch.md).
{: .warning }

Typically, **Params** are identifiers (e.g., file name, page number). Use them to fetch the task you assigned or write to a specific part of the shared store.

### Example

```python
# 1) Create a Node that uses params
class SummarizeFile(Node):
    def prep(self, shared):
        # Access the node's param
        filename = self.params["filename"]
        return shared["data"].get(filename, "")

    def exec(self, prep_res):
        prompt = f"Summarize: {prep_res}"
        return call_llm(prompt)

    def post(self, shared, prep_res, exec_res):
        filename = self.params["filename"]
        shared["summary"][filename] = exec_res
        return "default"

# 2) Set params
node = SummarizeFile()

# 3) Set Node params directly (for testing)
node.set_params({"filename": "doc1.txt"})
node.run(shared)

# 4) Create Flow
flow = Flow(start=node)

# 5) Set Flow params (overwrites node params)
flow.set_params({"filename": "doc2.txt"})
flow.run(shared)  # The node summarizes doc2, not doc1
```

================================================
File: docs/core_abstraction/flow.md
================================================
---
layout: default
title: "Flow"
parent: "Core Abstraction"
nav_order: 2
---

# Flow

A **Flow** orchestrates a graph of Nodes. You can chain Nodes in a sequence or create branching depending on the **Actions** returned from each Node's `post()`.

## 1. Action-based Transitions

Each Node's `post()` returns an **Action** string. By default, if `post()` doesn't return anything, we treat that as `"default"`.

You define transitions with the syntax:

1. **Basic default transition**: `node_a >> node_b`
  This means if `node_a.post()` returns `"default"`, go to `node_b`. 
  (Equivalent to `node_a - "default" >> node_b`)

2. **Named action transition**: `node_a - "action_name" >> node_b`
  This means if `node_a.post()` returns `"action_name"`, go to `node_b`.

It's possible to create loops, branching, or multi-step flows.

## 2. Creating a Flow

A **Flow** begins with a **start** node. You call `Flow(start=some_node)` to specify the entry point. When you call `flow.run(shared)`, it executes the start node, looks at its returned Action from `post()`, follows the transition, and continues until there's no next node.

### Example: Simple Sequence

Here's a minimal flow of two nodes in a chain:

```python
node_a >> node_b
flow = Flow(start=node_a)
flow.run(shared)
```

- When you run the flow, it executes `node_a`.  
- Suppose `node_a.post()` returns `"default"`.  
- The flow then sees `"default"` Action is linked to `node_b` and runs `node_b`.  
- `node_b.post()` returns `"default"` but we didn't define `node_b >> something_else`. So the flow ends there.

### Example: Branching & Looping

Here's a simple expense approval flow that demonstrates branching and looping. The `ReviewExpense` node can return three possible Actions:

- `"approved"`: expense is approved, move to payment processing
- `"needs_revision"`: expense needs changes, send back for revision 
- `"rejected"`: expense is denied, finish the process

We can wire them like this:

```python
# Define the flow connections
review - "approved" >> payment        # If approved, process payment
review - "needs_revision" >> revise   # If needs changes, go to revision
review - "rejected" >> finish         # If rejected, finish the process

revise >> review   # After revision, go back for another review
payment >> finish  # After payment, finish the process

flow = Flow(start=review)
```

Let's see how it flows:

1. If `review.post()` returns `"approved"`, the expense moves to the `payment` node
2. If `review.post()` returns `"needs_revision"`, it goes to the `revise` node, which then loops back to `review`
3. If `review.post()` returns `"rejected"`, it moves to the `finish` node and stops

```mermaid
flowchart TD
    review[Review Expense] -->|approved| payment[Process Payment]
    review -->|needs_revision| revise[Revise Report]
    review -->|rejected| finish[Finish Process]

    revise --> review
    payment --> finish
```

### Running Individual Nodes vs. Running a Flow

- `node.run(shared)`: Just runs that node alone (calls `prep->exec->post()`), returns an Action. 
- `flow.run(shared)`: Executes from the start node, follows Actions to the next node, and so on until the flow can't continue.

> `node.run(shared)` **does not** proceed to the successor.
> This is mainly for debugging or testing a single node.
> 
> Always use `flow.run(...)` in production to ensure the full pipeline runs correctly.
{: .warning }

## 3. Nested Flows

A **Flow** can act like a Node, which enables powerful composition patterns. This means you can:

1. Use a Flow as a Node within another Flow's transitions.  
2. Combine multiple smaller Flows into a larger Flow for reuse.  
3. Node `params` will be a merging of **all** parents' `params`.

### Flow's Node Methods

A **Flow** is also a **Node**, so it will run `prep()` and `post()`. However:

- It **won't** run `exec()`, as its main logic is to orchestrate its nodes.
- `post()` always receives `None` for `exec_res` and should instead get the flow execution results from the shared store.

### Basic Flow Nesting

Here's how to connect a flow to another node:

```python
# Create a sub-flow
node_a >> node_b
subflow = Flow(start=node_a)

# Connect it to another node
subflow >> node_c

# Create the parent flow
parent_flow = Flow(start=subflow)
```

When `parent_flow.run()` executes:
1. It starts `subflow`
2. `subflow` runs through its nodes (`node_a->node_b`)
3. After `subflow` completes, execution continues to `node_c`

### Example: Order Processing Pipeline

Here's a practical example that breaks down order processing into nested flows:

```python
# Payment processing sub-flow
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# Inventory sub-flow
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# Shipping sub-flow
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# Connect the flows into a main order pipeline
payment_flow >> inventory_flow >> shipping_flow

# Create the master flow
order_pipeline = Flow(start=payment_flow)

# Run the entire pipeline
order_pipeline.run(shared_data)
```

This creates a clean separation of concerns while maintaining a clear execution path:

```mermaid
flowchart LR
    subgraph order_pipeline[Order Pipeline]
        subgraph paymentFlow["Payment Flow"]
            A[Validate Payment] --> B[Process Payment] --> C[Payment Confirmation]
        end

        subgraph inventoryFlow["Inventory Flow"]
            D[Check Stock] --> E[Reserve Items] --> F[Update Inventory]
        end

        subgraph shippingFlow["Shipping Flow"]
            G[Create Label] --> H[Assign Carrier] --> I[Schedule Pickup]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

================================================
File: docs/core_abstraction/node.md
================================================
---
layout: default
title: "Node"
parent: "Core Abstraction"
nav_order: 1
---

# Node

A **Node** is the smallest building block. Each Node has 3 steps `prep->exec->post`:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`
   - **Read and preprocess data** from `shared` store. 
   - Examples: *query DB, read files, or serialize data into a string*.
   - Return `prep_res`, which is used by `exec()` and `post()`.

2. `exec(prep_res)`
   - **Execute compute logic**, with optional retries and error handling (below).
   - Examples: *(mostly) LLM calls, remote APIs, tool use*.
   - âš ï¸ This shall be only for compute and **NOT** access `shared`.
   - âš ï¸ If retries enabled, ensure idempotent implementation.
   - Return `exec_res`, which is passed to `post()`.

3. `post(shared, prep_res, exec_res)`
   - **Postprocess and write data** back to `shared`.
   - Examples: *update DB, change states, log results*.
   - **Decide the next action** by returning a *string* (`action = "default"` if *None*).

> **Why 3 steps?** To enforce the principle of *separation of concerns*. The data storage and data processing are operated separately.
>
> All steps are *optional*. E.g., you can only implement `prep` and `post` if you just need to process data.
{: .note }

### Fault Tolerance & Retries

You can **retry** `exec()` if it raises an exception via two parameters when define the Node:

- `max_retries` (int): Max times to run `exec()`. The default is `1` (**no** retry).
- `wait` (int): The time to wait (in **seconds**) before next retry. By default, `wait=0` (no waiting). 
`wait` is helpful when you encounter rate-limits or quota errors from your LLM provider and need to back off.

```python 
my_node = SummarizeFile(max_retries=3, wait=10)
```

When an exception occurs in `exec()`, the Node automatically retries until:

- It either succeeds, or
- The Node has retried `max_retries - 1` times already and fails on the last attempt.

You can get the current retry times (0-based) from `self.cur_retry`.

```python 
class RetryNode(Node):
    def exec(self, prep_res):
        print(f"Retry {self.cur_retry} times")
        raise Exception("Failed")
```

### Graceful Fallback

To **gracefully handle** the exception (after all retries) rather than raising it, override:

```python 
def exec_fallback(self, prep_res, exc):
    raise exc
```

By default, it just re-raises exception. But you can return a fallback result instead, which becomes the `exec_res` passed to `post()`.

### Example: Summarize file

```python 
class SummarizeFile(Node):
    def prep(self, shared):
        return shared["data"]

    def exec(self, prep_res):
        if not prep_res:
            return "Empty file content"
        prompt = f"Summarize this text in 10 words: {prep_res}"
        summary = call_llm(prompt)  # might fail
        return summary

    def exec_fallback(self, prep_res, exc):
        # Provide a simple fallback instead of crashing
        return "There was an error processing your request."

    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # Return "default" by not returning

summarize_node = SummarizeFile(max_retries=3)

# node.run() calls prep->exec->post
# If exec() fails, it retries up to 3 times before calling exec_fallback()
action_result = summarize_node.run(shared)

print("Action returned:", action_result)  # "default"
print("Summary stored:", shared["summary"])
```


================================================
File: docs/core_abstraction/parallel.md
================================================
---
layout: default
title: "(Advanced) Parallel"
parent: "Core Abstraction"
nav_order: 6
---

# (Advanced) Parallel

**Parallel** Nodes and Flows let you run multiple **Async** Nodes and Flows  **concurrently**â€”for example, summarizing multiple texts at once. This can improve performance by overlapping I/O and compute. 

> Because of Pythonâ€™s GIL, parallel nodes and flows canâ€™t truly parallelize CPU-bound tasks (e.g., heavy numerical computations). However, they excel at overlapping I/O-bound workâ€”like LLM calls, database queries, API requests, or file I/O.
{: .warning }

> - **Ensure Tasks Are Independent**: If each item depends on the output of a previous item, **do not** parallelize.
> 
> - **Beware of Rate Limits**: Parallel calls can **quickly** trigger rate limits on LLM services. You may need a **throttling** mechanism (e.g., semaphores or sleep intervals).
> 
> - **Consider Single-Node Batch APIs**: Some LLMs offer a **batch inference** API where you can send multiple prompts in a single call. This is more complex to implement but can be more efficient than launching many parallel requests and mitigates rate limits.
{: .best-practice }

## AsyncParallelBatchNode

Like **AsyncBatchNode**, but run `exec_async()` in **parallel**:

```python
class ParallelSummaries(AsyncParallelBatchNode):
    async def prep_async(self, shared):
        # e.g., multiple texts
        return shared["texts"]

    async def exec_async(self, text):
        prompt = f"Summarize: {text}"
        return await call_llm_async(prompt)

    async def post_async(self, shared, prep_res, exec_res_list):
        shared["summary"] = "\n\n".join(exec_res_list)
        return "default"

node = ParallelSummaries()
flow = AsyncFlow(start=node)
```

## AsyncParallelBatchFlow

Parallel version of **BatchFlow**. Each iteration of the sub-flow runs **concurrently** using different parameters:

```python
class SummarizeMultipleFiles(AsyncParallelBatchFlow):
    async def prep_async(self, shared):
        return [{"filename": f} for f in shared["files"]]

sub_flow = AsyncFlow(start=LoadAndSummarizeFile())
parallel_flow = SummarizeMultipleFiles(start=sub_flow)
await parallel_flow.run_async(shared)
```

================================================
File: docs/design_pattern/agent.md
================================================
---
layout: default
title: "Agent"
parent: "Design Pattern"
nav_order: 1
---

# Agent

Agent is a powerful design pattern in which nodes can take dynamic actions based on the context.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## Implement Agent with Graph

1. **Context and Action:** Implement nodes that supply context and perform actions.  
2. **Branching:** Use branching to connect each action node to an agent node. Use action to allow the agent to direct the [flow](mdc:../core_abstraction/flow.md) between nodesâ€”and potentially loop back for multi-step.
3. **Agent Node:** Provide a prompt to decide actionâ€”for example:

```python
f"""
### CONTEXT
Task: {task_description}
Previous Actions: {previous_actions}
Current State: {current_state}

### ACTION SPACE
[1] search
  Description: Use web search to get results
  Parameters:
    - query (str): What to search for

[2] answer
  Description: Conclude based on the results
  Parameters:
    - result (str): Final answer to provide

### NEXT ACTION
Decide the next action based on the current context and available action space.
Return your response in the following format:

```yaml
thinking: |
    <your step-by-step reasoning process>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
```"""
```

The core of building **high-performance** and **reliable** agents boils down to:

1. **Context Management:** Provide *relevant, minimal context.* For example, rather than including an entire chat history, retrieve the most relevant via [RAG](mdc:rag.md). Even with larger context windows, LLMs still fall victim to ["lost in the middle"](mdc:https:/arxiv.org/abs/2307.03172), overlooking mid-prompt content.

2. **Action Space:** Provide *a well-structured and unambiguous* set of actionsâ€”avoiding overlap like separate `read_databases` or  `read_csvs`. Instead, import CSVs into the database.

## Example Good Action Design

- **Incremental:** Feed content in manageable chunks (500 lines or 1 page) instead of all at once.

- **Overview-zoom-in:** First provide high-level structure (table of contents, summary), then allow drilling into details (raw texts).

- **Parameterized/Programmable:** Instead of fixed actions, enable parameterized (columns to select) or programmable (SQL queries) actions, for example, to read CSV files.

- **Backtracking:** Let the agent undo the last step instead of restarting entirely, preserving progress when encountering errors or dead ends.

## Example: Search Agent

This agent:
1. Decides whether to search or answer
2. If searches, loops back to decide if more search needed
3. Answers when enough context gathered

```python
class DecideAction(Node):
    def prep(self, shared):
        context = shared.get("context", "No previous search")
        query = shared["query"]
        return query, context
        
    def exec(self, inputs):
        query, context = inputs
        prompt = f"""
Given input: {query}
Previous search results: {context}
Should I: 1) Search web for more info 2) Answer with current knowledge
Output in yaml:
```yaml
action: search/answer
reason: why this action
search_term: search phrase if action is search
```"""
        resp = call_llm(prompt)
        yaml_str = resp.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        assert isinstance(result, dict)
        assert "action" in result
        assert "reason" in result
        assert result["action"] in ["search", "answer"]
        if result["action"] == "search":
            assert "search_term" in result
        
        return result

    def post(self, shared, prep_res, exec_res):
        if exec_res["action"] == "search":
            shared["search_term"] = exec_res["search_term"]
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        return shared["search_term"]
        
    def exec(self, search_term):
        return search_web(search_term)
    
    def post(self, shared, prep_res, exec_res):
        prev_searches = shared.get("context", [])
        shared["context"] = prev_searches + [
            {"term": shared["search_term"], "result": exec_res}
        ]
        return "decide"
        
class DirectAnswer(Node):
    def prep(self, shared):
        return shared["query"], shared.get("context", "")
        
    def exec(self, inputs):
        query, context = inputs
        return call_llm(f"Context: {context}\nAnswer: {query}")

    def post(self, shared, prep_res, exec_res):
       print(f"Answer: {exec_res}")
       shared["answer"] = exec_res

# Connect nodes
decide = DecideAction()
search = SearchWeb()
answer = DirectAnswer()

decide - "search" >> search
decide - "answer" >> answer
search - "decide" >> decide  # Loop back

flow = Flow(start=decide)
flow.run({"query": "Who won the Nobel Prize in Physics 2024?"})
```

================================================
File: docs/design_pattern/mapreduce.md
================================================
---
layout: default
title: "Map Reduce"
parent: "Design Pattern"
nav_order: 4
---

# Map Reduce

MapReduce is a design pattern suitable when you have either:
- Large input data (e.g., multiple files to process), or
- Large output data (e.g., multiple forms to fill)

and there is a logical way to break the task into smaller, ideally independent parts. 

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

You first break down the task using [BatchNode](mdc:../core_abstraction/batch.md) in the map phase, followed by aggregation in the reduce phase.

### Example: Document Summarization

```python
class SummarizeAllFiles(BatchNode):
    def prep(self, shared):
        files_dict = shared["files"]  # e.g. 10 files
        return list(files_dict.items())  # [("file1.txt", "aaa..."), ("file2.txt", "bbb..."), ...]

    def exec(self, one_file):
        filename, file_content = one_file
        summary_text = call_llm(f"Summarize the following file:\n{file_content}")
        return (filename, summary_text)

    def post(self, shared, prep_res, exec_res_list):
        shared["file_summaries"] = dict(exec_res_list)

class CombineSummaries(Node):
    def prep(self, shared):
        return shared["file_summaries"]

    def exec(self, file_summaries):
        # format as: "File1: summary\nFile2: summary...\n"
        text_list = []
        for fname, summ in file_summaries.items():
            text_list.append(f"{fname} summary:\n{summ}\n")
        big_text = "\n---\n".join(text_list)

        return call_llm(f"Combine these file summaries into one final summary:\n{big_text}")

    def post(self, shared, prep_res, final_summary):
        shared["all_files_summary"] = final_summary

batch_node = SummarizeAllFiles()
combine_node = CombineSummaries()
batch_node >> combine_node

flow = Flow(start=batch_node)

shared = {
    "files": {
        "file1.txt": "Alice was beginning to get very tired of sitting by her sister...",
        "file2.txt": "Some other interesting text ...",
        # ...
    }
}
flow.run(shared)
print("Individual Summaries:", shared["file_summaries"])
print("\nFinal Summary:\n", shared["all_files_summary"])
```

================================================
File: docs/design_pattern/rag.md
================================================
---
layout: default
title: "RAG"
parent: "Design Pattern"
nav_order: 3
---

# RAG (Retrieval Augmented Generation)

For certain LLM tasks like answering questions, providing relevant context is essential. One common architecture is a **two-stage** RAG pipeline:

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **Offline stage**: Preprocess and index documents ("building the index").
2. **Online stage**: Given a question, generate answers by retrieving the most relevant context.

---
## Stage 1: Offline Indexing

We create three Nodes:
1. `ChunkDocs` â€“ [chunks](mdc:../utility_function/chunking.md) raw text.
2. `EmbedDocs` â€“ [embeds](mdc:../utility_function/embedding.md) each chunk.
3. `StoreIndex` â€“ stores embeddings into a [vector database](mdc:../utility_function/vector.md).

```python
class ChunkDocs(BatchNode):
    def prep(self, shared):
        # A list of file paths in shared["files"]. We process each file.
        return shared["files"]

    def exec(self, filepath):
        # read file content. In real usage, do error handling.
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        # chunk by 100 chars each
        chunks = []
        size = 100
        for i in range(0, len(text), size):
            chunks.append(text[i : i + size])
        return chunks
    
    def post(self, shared, prep_res, exec_res_list):
        # exec_res_list is a list of chunk-lists, one per file.
        # flatten them all into a single list of chunks.
        all_chunks = []
        for chunk_list in exec_res_list:
            all_chunks.extend(chunk_list)
        shared["all_chunks"] = all_chunks

class EmbedDocs(BatchNode):
    def prep(self, shared):
        return shared["all_chunks"]

    def exec(self, chunk):
        return get_embedding(chunk)

    def post(self, shared, prep_res, exec_res_list):
        # Store the list of embeddings.
        shared["all_embeds"] = exec_res_list
        print(f"Total embeddings: {len(exec_res_list)}")

class StoreIndex(Node):
    def prep(self, shared):
        # We'll read all embeds from shared.
        return shared["all_embeds"]

    def exec(self, all_embeds):
        # Create a vector index (faiss or other DB in real usage).
        index = create_index(all_embeds)
        return index

    def post(self, shared, prep_res, index):
        shared["index"] = index

# Wire them in sequence
chunk_node = ChunkDocs()
embed_node = EmbedDocs()
store_node = StoreIndex()

chunk_node >> embed_node >> store_node

OfflineFlow = Flow(start=chunk_node)
```

Usage example:

```python
shared = {
    "files": ["doc1.txt", "doc2.txt"],  # any text files
}
OfflineFlow.run(shared)
```

---
## Stage 2: Online Query & Answer

We have 3 nodes:
1. `EmbedQuery` â€“ embeds the userâ€™s question.
2. `RetrieveDocs` â€“ retrieves top chunk from the index.
3. `GenerateAnswer` â€“ calls the LLM with the question + chunk to produce the final answer.

```python
class EmbedQuery(Node):
    def prep(self, shared):
        return shared["question"]

    def exec(self, question):
        return get_embedding(question)

    def post(self, shared, prep_res, q_emb):
        shared["q_emb"] = q_emb

class RetrieveDocs(Node):
    def prep(self, shared):
        # We'll need the query embedding, plus the offline index/chunks
        return shared["q_emb"], shared["index"], shared["all_chunks"]

    def exec(self, inputs):
        q_emb, index, chunks = inputs
        I, D = search_index(index, q_emb, top_k=1)
        best_id = I[0][0]
        relevant_chunk = chunks[best_id]
        return relevant_chunk

    def post(self, shared, prep_res, relevant_chunk):
        shared["retrieved_chunk"] = relevant_chunk
        print("Retrieved chunk:", relevant_chunk[:60], "...")

class GenerateAnswer(Node):
    def prep(self, shared):
        return shared["question"], shared["retrieved_chunk"]

    def exec(self, inputs):
        question, chunk = inputs
        prompt = f"Question: {question}\nContext: {chunk}\nAnswer:"
        return call_llm(prompt)

    def post(self, shared, prep_res, answer):
        shared["answer"] = answer
        print("Answer:", answer)

embed_qnode = EmbedQuery()
retrieve_node = RetrieveDocs()
generate_node = GenerateAnswer()

embed_qnode >> retrieve_node >> generate_node
OnlineFlow = Flow(start=embed_qnode)
```

Usage example:

```python
# Suppose we already ran OfflineFlow and have:
# shared["all_chunks"], shared["index"], etc.
shared["question"] = "Why do people like cats?"

OnlineFlow.run(shared)
# final answer in shared["answer"]
```

================================================
File: docs/design_pattern/structure.md
================================================
---
layout: default
title: "Structured Output"
parent: "Design Pattern"
nav_order: 5
---

# Structured Output

In many use cases, you may want the LLM to output a specific structure, such as a list or a dictionary with predefined keys.

There are several approaches to achieve a structured output:
- **Prompting** the LLM to strictly return a defined structure.
- Using LLMs that natively support **schema enforcement**.
- **Post-processing** the LLM's response to extract structured content.

In practice, **Prompting** is simple and reliable for modern LLMs.

### Example Use Cases

- Extracting Key Information 

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    A high-quality widget designed for professionals.
    Recommended for advanced users.
```

- Summarizing Documents into Bullet Points

```yaml
summary:
  - This product is easy to use.
  - It is cost-effective.
  - Suitable for all skill levels.
```

- Generating Configuration Files

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## Prompt Engineering

When prompting the LLM to produce **structured** output:
1. **Wrap** the structure in code fences (e.g., `yaml`).
2. **Validate** that all required fields exist (and let `Node` handles retry).

### Example Text Summarization

```python
class SummarizeNode(Node):
    def exec(self, prep_res):
        # Suppose `prep_res` is the text to summarize.
        prompt = f"""
Please summarize the following text as YAML, with exactly 3 bullet points

{prep_res}

Now, output:
```yaml
summary:
  - bullet 1
  - bullet 2
  - bullet 3
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()

        import yaml
        structured_result = yaml.safe_load(yaml_str)

        assert "summary" in structured_result
        assert isinstance(structured_result["summary"], list)

        return structured_result
```

> Besides using `assert` statements, another popular way to validate schemas is [Pydantic](mdc:https:/github.com/pydantic/pydantic)
{: .note }

### Why YAML instead of JSON?

Current LLMs struggle with escaping. YAML is easier with strings since they don't always need quotes.

**In JSON**  

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- Every double quote inside the string must be escaped with `\"`.
- Each newline in the dialogue must be represented as `\n`.

**In YAML**  

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- No need to escape interior quotesâ€”just place the entire text under a block literal (`|`).
- Newlines are naturally preserved without needing `\n`.

================================================
File: docs/design_pattern/workflow.md
================================================
---
layout: default
title: "Workflow"
parent: "Design Pattern"
nav_order: 2
---

# Workflow

Many real-world tasks are too complex for one LLM call. The solution is to **Task Decomposition**: decompose them into a [chain](mdc:../core_abstraction/flow.md) of multiple Nodes.

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - You don't want to make each task **too coarse**, because it may be *too complex for one LLM call*.
> - You don't want to make each task **too granular**, because then *the LLM call doesn't have enough context* and results are *not consistent across nodes*.
> 
> You usually need multiple *iterations* to find the *sweet spot*. If the task has too many *edge cases*, consider using [Agents](mdc:agent.md).
{: .best-practice }

### Example: Article Writing

```python
class GenerateOutline(Node):
    def prep(self, shared): return shared["topic"]
    def exec(self, topic): return call_llm(f"Create a detailed outline for an article about {topic}")
    def post(self, shared, prep_res, exec_res): shared["outline"] = exec_res

class WriteSection(Node):
    def prep(self, shared): return shared["outline"]
    def exec(self, outline): return call_llm(f"Write content based on this outline: {outline}")
    def post(self, shared, prep_res, exec_res): shared["draft"] = exec_res

class ReviewAndRefine(Node):
    def prep(self, shared): return shared["draft"]
    def exec(self, draft): return call_llm(f"Review and improve this draft: {draft}")
    def post(self, shared, prep_res, exec_res): shared["final_article"] = exec_res

# Connect nodes
outline = GenerateOutline()
write = WriteSection()
review = ReviewAndRefine()

outline >> write >> review

# Create and run flow
writing_flow = Flow(start=outline)
shared = {"topic": "AI Safety"}
writing_flow.run(shared)
```

For *dynamic cases*, consider using [Agents](mdc:agent.md).

================================================
File: docs/utility_function/llm.md
================================================
---
layout: default
title: "LLM Wrapper"
parent: "Utility Function"
nav_order: 1
---

# LLM Wrappers

Check out libraries like [litellm](mdc:https:/github.com/BerriAI/litellm). 
Here, we provide some minimal example implementations:

1. OpenAI
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_API_KEY_HERE")
        r = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content

    # Example usage
    call_llm("How are you?")
    ```
    > Store the API key in an environment variable like OPENAI_API_KEY for security.
    {: .best-practice }

2. Claude (Anthropic)
    ```python
    def call_llm(prompt):
        from anthropic import Anthropic
        client = Anthropic(api_key="YOUR_API_KEY_HERE")
        response = client.messages.create(
            model="claude-2",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100
        )
        return response.content
    ```

3. Google (Generative AI Studio / PaLM API)
    ```python
    def call_llm(prompt):
        import google.generativeai as genai
        genai.configure(api_key="YOUR_API_KEY_HERE")
        response = genai.generate_text(
            model="models/text-bison-001",
            prompt=prompt
        )
        return response.result
    ```

4. Azure (Azure OpenAI)
    ```python
    def call_llm(prompt):
        from openai import AzureOpenAI
        client = AzureOpenAI(
            azure_endpoint="https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
            api_key="YOUR_API_KEY_HERE",
            api_version="2023-05-15"
        )
        r = client.chat.completions.create(
            model="<YOUR_DEPLOYMENT_NAME>",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```

5. Ollama (Local LLM)
    ```python
    def call_llm(prompt):
        from ollama import chat
        response = chat(
            model="llama2",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.message.content
    ```

## Improvements
Feel free to enhance your `call_llm` function as needed. Here are examples:

- Handle chat history:

```python
def call_llm(messages):
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return r.choices[0].message.content
```

- Add in-memory caching 

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def call_llm(prompt):
    # Your implementation here
    pass
```

> âš ï¸ Caching conflicts with Node retries, as retries yield the same result.
>
> To address this, you could use cached results only if not retried.
{: .warning }


```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_call(prompt):
    pass

def call_llm(prompt, use_cache):
    if use_cache:
        return cached_call(prompt)
    # Call the underlying function directly
    return cached_call.__wrapped__(prompt)

class SummarizeNode(Node):
    def exec(self, text):
        return call_llm(f"Summarize: {text}", self.cur_retry==0)
```

- Enable logging:

```python
def call_llm(prompt):
    import logging
    logging.info(f"Prompt: {prompt}")
    response = ... # Your implementation here
    logging.info(f"Response: {response}")
    return response
```


# æ™ºèƒ½ä½“ç¼–ç ï¼šäººç±»è®¾è®¡ï¼Œæ™ºèƒ½ä½“ç¼–ç ï¼

> å¦‚æžœä½ æ˜¯å‚ä¸Žæž„å»ºå¤§è¯­è¨€æ¨¡åž‹ç³»ç»Ÿçš„AIæ™ºèƒ½ä½“ï¼Œè¯·**éžå¸¸ã€éžå¸¸ä»”ç»†**åœ°é˜…è¯»æœ¬æŒ‡å—ï¼è¿™æ˜¯æ•´ä¸ªæ–‡æ¡£ä¸­æœ€é‡è¦çš„ç« èŠ‚ã€‚åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œä½ åº”è¯¥å§‹ç»ˆï¼ˆ1ï¼‰ä»Žä¸€ä¸ªå°è€Œç®€å•çš„è§£å†³æ–¹æ¡ˆå¼€å§‹ï¼Œï¼ˆ2ï¼‰åœ¨å®žçŽ°å‰è¿›è¡Œé«˜å±‚è®¾è®¡ï¼ˆ`docs/design.md`ï¼‰ï¼Œï¼ˆ3ï¼‰é¢‘ç¹å‘äººç±»å¯»æ±‚åé¦ˆå’Œæ¾„æ¸…ã€‚
{: .warning }

## æ™ºèƒ½ä½“ç¼–ç æ­¥éª¤

æ™ºèƒ½ä½“ç¼–ç åº”è¯¥æ˜¯äººç±»ç³»ç»Ÿè®¾è®¡å’Œæ™ºèƒ½ä½“å®žçŽ°ä¹‹é—´çš„åä½œï¼š

| æ­¥éª¤ | äººç±» | AI | è¯´æ˜Ž |
|:-----------------------|:----------:|:---------:|:------------------------------------------------------------------------|
| 1. éœ€æ±‚ | â˜…â˜…â˜… é«˜ | â˜…â˜†â˜† ä½Ž | äººç±»ç†è§£éœ€æ±‚å’ŒèƒŒæ™¯ã€‚ |
| 2. æµç¨‹ | â˜…â˜…â˜† ä¸­ | â˜…â˜…â˜† ä¸­ | äººç±»æŒ‡å®šé«˜å±‚è®¾è®¡ï¼ŒAIè¡¥å……ç»†èŠ‚ã€‚ |
| 3. å·¥å…· | â˜…â˜…â˜† ä¸­ | â˜…â˜…â˜† ä¸­ | äººç±»æä¾›å¯ç”¨çš„å¤–éƒ¨APIå’Œé›†æˆï¼ŒAIååŠ©å®žçŽ°ã€‚ |
| 4. èŠ‚ç‚¹ | â˜…â˜†â˜† ä½Ž | â˜…â˜…â˜… é«˜ | AIåŸºäºŽæµç¨‹å¸®åŠ©è®¾è®¡èŠ‚ç‚¹ç±»åž‹å’Œæ•°æ®å¤„ç†æ–¹å¼ã€‚ |
| 5. å®žçŽ° | â˜…â˜†â˜† ä½Ž | â˜…â˜…â˜… é«˜ | AIåŸºäºŽè®¾è®¡å®žçŽ°æµç¨‹ã€‚ |
| 6. ä¼˜åŒ– | â˜…â˜…â˜† ä¸­ | â˜…â˜…â˜† ä¸­ | äººç±»è¯„ä¼°ç»“æžœï¼ŒAIååŠ©ä¼˜åŒ–ã€‚ |
| 7. å¯é æ€§ | â˜…â˜†â˜† ä½Ž | â˜…â˜…â˜… é«˜ | AIç¼–å†™æµ‹è¯•ç”¨ä¾‹å¹¶å¤„ç†è¾¹ç¼˜æƒ…å†µã€‚ |

1. **éœ€æ±‚**ï¼šæ˜Žç¡®é¡¹ç›®éœ€æ±‚ï¼Œè¯„ä¼°AIç³»ç»Ÿæ˜¯å¦é€‚ç”¨ã€‚
    - ç†è§£AIç³»ç»Ÿçš„ä¼˜åŠ¿å’Œå±€é™æ€§ï¼š
      - **æ“…é•¿**ï¼šéœ€è¦å¸¸è¯†çš„å¸¸è§„ä»»åŠ¡ï¼ˆå¡«å†™è¡¨å•ã€å›žå¤é‚®ä»¶ï¼‰
      - **æ“…é•¿**ï¼šè¾“å…¥æ˜Žç¡®çš„åˆ›é€ æ€§ä»»åŠ¡ï¼ˆåˆ¶ä½œå¹»ç¯ç‰‡ã€ç¼–å†™SQLï¼‰
      - **ä¸æ“…é•¿**ï¼šéœ€è¦å¤æ‚å†³ç­–çš„æ¨¡ç³Šé—®é¢˜ï¼ˆå•†ä¸šæˆ˜ç•¥ã€åˆ›ä¸šè§„åˆ’ï¼‰
    - **ä»¥ç”¨æˆ·ä¸ºä¸­å¿ƒ**ï¼šä»Žç”¨æˆ·è§’åº¦è§£é‡Šâ€œé—®é¢˜â€ï¼Œè€Œä¸ä»…ä»…æ˜¯åˆ—å‡ºåŠŸèƒ½ã€‚
    - **å¹³è¡¡å¤æ‚æ€§ä¸Žå½±å“**ï¼šå°½æ—©äº¤ä»˜é«˜ä»·å€¼ã€ä½Žå¤æ‚åº¦çš„åŠŸèƒ½ã€‚

2. **æµç¨‹è®¾è®¡**ï¼šä»Žé«˜å±‚æ¦‚è¿°ï¼Œæè¿°AIç³»ç»Ÿå¦‚ä½•ç¼–æŽ’èŠ‚ç‚¹ã€‚
    - ç¡®å®šé€‚ç”¨çš„è®¾è®¡æ¨¡å¼ï¼ˆå¦‚[Map Reduce](mdc:design_pattern/mapreduce.md)ã€[æ™ºèƒ½ä½“](mdc:design_pattern/agent.md)ã€[RAG](mdc:design_pattern/rag.md)ï¼‰ã€‚
      - å¯¹äºŽæµç¨‹ä¸­çš„æ¯ä¸ªèŠ‚ç‚¹ï¼Œå…ˆç”¨ä¸€å¥è¯é«˜å±‚æè¿°å…¶åŠŸèƒ½ã€‚
      - å¦‚æžœä½¿ç”¨**Map Reduce**ï¼Œæ˜Žç¡®å¦‚ä½•æ˜ å°„ï¼ˆæ‹†åˆ†å†…å®¹ï¼‰å’Œå½’çº¦ï¼ˆåˆå¹¶ç»“æžœï¼‰ã€‚
      - å¦‚æžœä½¿ç”¨**æ™ºèƒ½ä½“**ï¼Œæ˜Žç¡®è¾“å…¥ï¼ˆä¸Šä¸‹æ–‡ï¼‰å’Œå¯èƒ½çš„åŠ¨ä½œã€‚
      - å¦‚æžœä½¿ç”¨**RAG**ï¼Œæ˜Žç¡®åµŒå…¥å†…å®¹ï¼Œæ³¨æ„é€šå¸¸åŒ…å«ç¦»çº¿ï¼ˆç´¢å¼•ï¼‰å’Œåœ¨çº¿ï¼ˆæ£€ç´¢ï¼‰å·¥ä½œæµã€‚
    - æ¦‚è¿°æµç¨‹å¹¶ç”¨mermaidå›¾ç»˜åˆ¶ã€‚ä¾‹å¦‚ï¼š
      ```mermaid
      flowchart LR
          start[å¼€å§‹] --> batch[æ‰¹é‡å¤„ç†]
          batch --> check[æ£€æŸ¥]
          check -->|æ­£å¸¸| process[å¤„ç†]
          check -->|é”™è¯¯| fix[ä¿®å¤]
          fix --> check
          
          subgraph process[å¤„ç†]
            step1[æ­¥éª¤1] --> step2[æ­¥éª¤2]
          end
          
          process --> endNode[ç»“æŸ]
      ```
    - > **å¦‚æžœäººç±»æ— æ³•æ˜Žç¡®æµç¨‹ï¼ŒAIæ™ºèƒ½ä½“å°±æ— æ³•è‡ªåŠ¨åŒ–å®ƒï¼** åœ¨æž„å»ºå¤§è¯­è¨€æ¨¡åž‹ç³»ç»Ÿå‰ï¼Œé€šè¿‡æ‰‹åŠ¨è§£å†³ç¤ºä¾‹è¾“å…¥æ¥å½»åº•ç†è§£é—®é¢˜å’Œæ½œåœ¨è§£å†³æ–¹æ¡ˆï¼ŒåŸ¹å…»ç›´è§‰ã€‚
      {: .best-practice }

3. **å·¥å…·**ï¼šåŸºäºŽæµç¨‹è®¾è®¡ï¼Œç¡®å®šå¹¶å®žçŽ°å¿…è¦çš„å·¥å…·å‡½æ•°ã€‚
    - å¯ä»¥å°†AIç³»ç»Ÿè§†ä¸ºå¤§è„‘ã€‚å®ƒéœ€è¦ä¸€ä¸ªâ€œèº«ä½“â€â€”â€”è¿™äº›**å¤–éƒ¨å·¥å…·å‡½æ•°**â€”â€”æ¥ä¸ŽçŽ°å®žä¸–ç•Œäº¤äº’ï¼š
        <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/utility.png?raw=true" width="400"/></div>

        - è¯»å–è¾“å…¥ï¼ˆå¦‚èŽ·å–Slackæ¶ˆæ¯ã€è¯»å–é‚®ä»¶ï¼‰
        - å†™å…¥è¾“å‡ºï¼ˆå¦‚ç”ŸæˆæŠ¥å‘Šã€å‘é€é‚®ä»¶ï¼‰
        - ä½¿ç”¨å¤–éƒ¨å·¥å…·ï¼ˆå¦‚è°ƒç”¨å¤§è¯­è¨€æ¨¡åž‹ã€ç½‘é¡µæœç´¢ï¼‰
        - **æ³¨æ„**ï¼š**åŸºäºŽå¤§è¯­è¨€æ¨¡åž‹çš„ä»»åŠ¡**ï¼ˆå¦‚æ–‡æœ¬æ‘˜è¦ã€æƒ…æ„Ÿåˆ†æžï¼‰ä¸æ˜¯å·¥å…·å‡½æ•°ï¼Œè€Œæ˜¯AIç³»ç»Ÿçš„**æ ¸å¿ƒåŠŸèƒ½**ã€‚
    - ä¸ºæ¯ä¸ªå·¥å…·å‡½æ•°å®žçŽ°å¹¶ç¼–å†™ç®€å•æµ‹è¯•ã€‚
    - è®°å½•å…¶è¾“å…¥/è¾“å‡ºä»¥åŠå¿…è¦æ€§ã€‚ä¾‹å¦‚ï¼š
      - `åç§°`ï¼š`get_embedding`ï¼ˆ`utils/get_embedding.py`ï¼‰
      - `è¾“å…¥`ï¼š`str`ï¼ˆå­—ç¬¦ä¸²ï¼‰
      - `è¾“å‡º`ï¼š3072ç»´æµ®ç‚¹æ•°å‘é‡
      - `å¿…è¦æ€§`ï¼šä¾›ç¬¬äºŒä¸ªèŠ‚ç‚¹åµŒå…¥æ–‡æœ¬ä½¿ç”¨
    - å·¥å…·å®žçŽ°ç¤ºä¾‹ï¼š
      ```python
      # utils/call_llm.py
      from openai import OpenAI

      def call_llm(prompt):    
          client = OpenAI(api_key="YOUR_API_KEY_HERE")
          r = client.chat.completions.create(
              model="gpt-4o",
              messages=[{"role": "user", "content": prompt}]
          )
          return r.choices[0].message.content
          
      if __name__ == "__main__":
          prompt = "ç”Ÿå‘½çš„æ„ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ"
          print(call_llm(prompt))
      ```
    - > **æœ‰æ—¶ï¼Œå…ˆè®¾è®¡å·¥å…·å†è®¾è®¡æµç¨‹**ï¼šä¾‹å¦‚ï¼Œå¯¹äºŽè‡ªåŠ¨åŒ–é—ç•™ç³»ç»Ÿçš„å¤§è¯­è¨€æ¨¡åž‹é¡¹ç›®ï¼Œç“¶é¢ˆå¯èƒ½æ˜¯ä¸Žè¯¥ç³»ç»Ÿçš„å¯ç”¨æŽ¥å£ã€‚å…ˆè®¾è®¡æœ€éš¾çš„æŽ¥å£å·¥å…·ï¼Œå†å›´ç»•å®ƒä»¬æž„å»ºæµç¨‹ã€‚
      {: .best-practice }

4. **èŠ‚ç‚¹è®¾è®¡**ï¼šè§„åˆ’æ¯ä¸ªèŠ‚ç‚¹å¦‚ä½•è¯»å†™æ•°æ®ä»¥åŠä½¿ç”¨å·¥å…·å‡½æ•°ã€‚
   - PocketFlowçš„æ ¸å¿ƒè®¾è®¡åŽŸåˆ™ä¹‹ä¸€æ˜¯ä½¿ç”¨[å…±äº«å­˜å‚¨](mdc:core_abstraction/communication.md)ï¼Œå› æ­¤å…ˆä»Žå…±äº«å­˜å‚¨è®¾è®¡å¼€å§‹ï¼š
      - å¯¹äºŽç®€å•ç³»ç»Ÿï¼Œä½¿ç”¨å†…å­˜å­—å…¸ã€‚
      - å¯¹äºŽæ›´å¤æ‚çš„ç³»ç»Ÿæˆ–éœ€è¦æŒä¹…åŒ–æ—¶ï¼Œä½¿ç”¨æ•°æ®åº“ã€‚
      - **é¿å…é‡å¤**ï¼šä½¿ç”¨å†…å­˜å¼•ç”¨æˆ–å¤–é”®ã€‚
      - å…±äº«å­˜å‚¨è®¾è®¡ç¤ºä¾‹ï¼š
        ```python
        shared = {
            "user": {
                "id": "user123",
                "context": {                # åµŒå¥—å­—å…¸
                    "weather": {"temp": 72, "condition": "æ™´å¤©"},
                    "location": "æ—§é‡‘å±±"
                }
            },
            "results": {}                   # å­˜å‚¨è¾“å‡ºçš„ç©ºå­—å…¸
        }
        ```
   - å¯¹äºŽæ¯ä¸ª[èŠ‚ç‚¹](mdc:core_abstraction/node.md)ï¼Œæè¿°å…¶ç±»åž‹ã€è¯»å†™æ•°æ®çš„æ–¹å¼ä»¥åŠä½¿ç”¨çš„å·¥å…·å‡½æ•°ã€‚ä¿æŒå…·ä½“ä½†é«˜å±‚ï¼Œä¸æ¶‰åŠä»£ç ã€‚ä¾‹å¦‚ï¼š
     - `ç±»åž‹`ï¼šå¸¸è§„ï¼ˆæˆ–æ‰¹é‡ã€å¼‚æ­¥ï¼‰
     - `å‡†å¤‡`ï¼šä»Žå…±äº«å­˜å‚¨è¯»å–â€œtextâ€
     - `æ‰§è¡Œ`ï¼šè°ƒç”¨åµŒå…¥å·¥å…·å‡½æ•°
     - `åŽç½®å¤„ç†`ï¼šå°†â€œembeddingâ€å†™å…¥å…±äº«å­˜å‚¨

5. **å®žçŽ°**ï¼šåŸºäºŽè®¾è®¡å®žçŽ°åˆå§‹èŠ‚ç‚¹å’Œæµç¨‹ã€‚
   - ðŸŽ‰ å¦‚æžœåˆ°è¾¾æ­¤æ­¥éª¤ï¼Œäººç±»å·²å®Œæˆè®¾è®¡ã€‚çŽ°åœ¨å¼€å§‹**æ™ºèƒ½ä½“ç¼–ç **ï¼
   - **â€œä¿æŒç®€å•ï¼â€** é¿å…å¤æ‚åŠŸèƒ½å’Œå…¨é¢çš„ç±»åž‹æ£€æŸ¥ã€‚
   - **å¿«é€Ÿå¤±è´¥**ï¼é¿å…`try`é€»è¾‘ï¼Œä»¥ä¾¿å¿«é€Ÿè¯†åˆ«ç³»ç»Ÿä¸­çš„å¼±ç‚¹ã€‚
   - åœ¨ä»£ç ä¸­æ·»åŠ æ—¥å¿—ä»¥ä¾¿è°ƒè¯•ã€‚

7. **ä¼˜åŒ–**ï¼š
   - **åˆ©ç”¨ç›´è§‰**ï¼šå¯¹äºŽå¿«é€Ÿåˆå§‹è¯„ä¼°ï¼Œäººç±»ç›´è§‰é€šå¸¸æ˜¯ä¸ªå¥½å¼€å§‹ã€‚
   - **é‡æ–°è®¾è®¡æµç¨‹ï¼ˆå›žåˆ°æ­¥éª¤3ï¼‰**ï¼šè€ƒè™‘è¿›ä¸€æ­¥æ‹†åˆ†ä»»åŠ¡ã€å¼•å…¥æ™ºèƒ½ä½“å†³ç­–æˆ–æ›´å¥½åœ°ç®¡ç†è¾“å…¥ä¸Šä¸‹æ–‡ã€‚
   - å¦‚æžœæµç¨‹è®¾è®¡å·²ç»å¯é ï¼Œè¿›è¡Œå¾®è§‚ä¼˜åŒ–ï¼š
     - **æç¤ºå·¥ç¨‹**ï¼šä½¿ç”¨æ¸…æ™°ã€å…·ä½“çš„æŒ‡ä»¤å’Œç¤ºä¾‹å‡å°‘æ­§ä¹‰ã€‚
     - **ä¸Šä¸‹æ–‡å­¦ä¹ **ï¼šå¯¹äºŽéš¾ä»¥ç”¨æŒ‡ä»¤æ˜Žç¡®çš„ä»»åŠ¡ï¼Œæä¾›å¯é ç¤ºä¾‹ã€‚

   - > **ä½ å¯èƒ½éœ€è¦å¤§é‡è¿­ä»£ï¼** é¢„è®¡é‡å¤æ­¥éª¤3-6æ•°ç™¾æ¬¡ã€‚
     >
     > <div align="center"><img src="https://github.com/the-pocket/.github/raw/main/assets/success.png?raw=true" width="400"/></div>
     {: .best-practice }

8. **å¯é æ€§**  
   - **èŠ‚ç‚¹é‡è¯•**ï¼šåœ¨èŠ‚ç‚¹`exec`ä¸­æ·»åŠ æ£€æŸ¥ï¼Œç¡®ä¿è¾“å‡ºç¬¦åˆè¦æ±‚ï¼Œå¹¶è€ƒè™‘å¢žåŠ `max_retries`ï¼ˆæœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰å’Œ`wait`ï¼ˆç­‰å¾…æ—¶é—´ï¼‰ã€‚
   - **æ—¥å¿—å’Œå¯è§†åŒ–**ï¼šä¿ç•™æ‰€æœ‰å°è¯•çš„æ—¥å¿—ï¼Œå¹¶å¯è§†åŒ–èŠ‚ç‚¹ç»“æžœä»¥ä¾¿è°ƒè¯•ã€‚
   - **è‡ªæˆ‘è¯„ä¼°**ï¼šæ·»åŠ å•ç‹¬çš„èŠ‚ç‚¹ï¼ˆç”±å¤§è¯­è¨€æ¨¡åž‹é©±åŠ¨ï¼‰ï¼Œåœ¨ç»“æžœä¸ç¡®å®šæ—¶å®¡æŸ¥è¾“å‡ºã€‚

## å¤§è¯­è¨€æ¨¡åž‹é¡¹ç›®æ–‡ä»¶ç»“æž„ç¤ºä¾‹

```
my_project/
â”œâ”€â”€ main.py
â”œâ”€â”€ nodes.py
â”œâ”€â”€ flow.py
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ call_llm.py
â”‚   â””â”€â”€ search_web.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ docs/
    â””â”€â”€ design.md
```

- **`docs/design.md`**ï¼šåŒ…å«ä¸Šè¿°æ¯ä¸ªæ­¥éª¤çš„é¡¹ç›®æ–‡æ¡£ã€‚åº”**é«˜å±‚**ä¸”**æ— ä»£ç **ã€‚
- **`utils/`**ï¼šåŒ…å«æ‰€æœ‰å·¥å…·å‡½æ•°ã€‚
  - å»ºè®®ä¸ºæ¯ä¸ªAPIè°ƒç”¨å•ç‹¬åˆ›å»ºä¸€ä¸ªPythonæ–‡ä»¶ï¼Œä¾‹å¦‚`call_llm.py`æˆ–`search_web.py`ã€‚
  - æ¯ä¸ªæ–‡ä»¶è¿˜åº”åŒ…å«`main()`å‡½æ•°ä»¥æµ‹è¯•è¯¥APIè°ƒç”¨
- **`nodes.py`**ï¼šåŒ…å«æ‰€æœ‰èŠ‚ç‚¹å®šä¹‰ã€‚
  ```python
  # nodes.py
  from pocketflow import Node
  from utils.call_llm import call_llm

  class GetQuestionNode(Node):
      def exec(self, _):
          # ç›´æŽ¥ä»Žç”¨æˆ·è¾“å…¥èŽ·å–é—®é¢˜
          user_question = input("è¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
          return user_question
      
      def post(self, shared, prep_res, exec_res):
          # å­˜å‚¨ç”¨æˆ·çš„é—®é¢˜
          shared["question"] = exec_res
          return "default"  # è¿›å…¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹

  class AnswerNode(Node):
      def prep(self, shared):
          # ä»Žå…±äº«å­˜å‚¨è¯»å–é—®é¢˜
          return shared["question"]
      
      def exec(self, question):
          # è°ƒç”¨å¤§è¯­è¨€æ¨¡åž‹èŽ·å–ç­”æ¡ˆ
          return call_llm(question)
      
      def post(self, shared, prep_res, exec_res):
          # å°†ç­”æ¡ˆå­˜å‚¨åˆ°å…±äº«å­˜å‚¨
          shared["answer"] = exec_res
  ```
- **`flow.py`**ï¼šå®žçŽ°é€šè¿‡å¯¼å…¥èŠ‚ç‚¹å®šä¹‰å¹¶è¿žæŽ¥å®ƒä»¬æ¥åˆ›å»ºæµç¨‹çš„å‡½æ•°ã€‚
  ```python
  # flow.py
  from pocketflow import Flow
  from nodes import GetQuestionNode, AnswerNode

  def create_qa_flow():
      """åˆ›å»ºå¹¶è¿”å›žé—®ç­”æµç¨‹ã€‚"""
      # åˆ›å»ºèŠ‚ç‚¹
      get_question_node = GetQuestionNode()
      answer_node = AnswerNode()
      
      # æŒ‰é¡ºåºè¿žæŽ¥èŠ‚ç‚¹
      get_question_node >> answer_node
      
      # åˆ›å»ºä»¥è¾“å…¥èŠ‚ç‚¹ä¸ºèµ·ç‚¹çš„æµç¨‹
      return Flow(start=get_question_node)
  ```
- **`main.py`**ï¼šä½œä¸ºé¡¹ç›®çš„å…¥å£ç‚¹ã€‚
  ```python
  # main.py
  from flow import create_qa_flow

  # ç¤ºä¾‹ä¸»å‡½æ•°
  # è¯·æ›¿æ¢ä¸ºä½ è‡ªå·±çš„ä¸»å‡½æ•°
  def main():
      shared = {
          "question": None,  # å°†ç”±GetQuestionNodeä»Žç”¨æˆ·è¾“å…¥å¡«å……
          "answer": None     # å°†ç”±AnswerNodeå¡«å……
      }

      # åˆ›å»ºæµç¨‹å¹¶è¿è¡Œ
      qa_flow = create_qa_flow()
      qa_flow.run(shared)
      print(f"é—®é¢˜ï¼š{shared['question']}")
      print(f"ç­”æ¡ˆï¼š{shared['answer']}")

  if __name__ == "__main__":
      main()
  ```

================================================
File: docs/index.md
================================================
---
layout: default
title: "é¦–é¡µ"
nav_order: 1
---

# Pocket Flow

ä¸€ä¸ª[100è¡Œ](mdc:https:/github.com/the-pocket/PocketFlow/blob/main/pocketflow/__init__.py)çš„æžç®€å¤§è¯­è¨€æ¨¡åž‹æ¡†æž¶ï¼Œç”¨äºŽ**æ™ºèƒ½ä½“ã€ä»»åŠ¡åˆ†è§£ã€RAGç­‰**ã€‚

- **è½»é‡**ï¼šæ ¸å¿ƒå›¾æŠ½è±¡ä»…100è¡Œã€‚é›¶ä¾èµ–ï¼Œæ— åŽ‚å•†é”å®šã€‚
- ** expressive**ï¼šåŒ…å«å¤§åž‹æ¡†æž¶çš„æ‰€æœ‰ä¼˜åŠ¿â€”â€”([å¤š](mdc:design_pattern/multi_agent.html))[æ™ºèƒ½ä½“](mdc:design_pattern/agent.html)ã€[å·¥ä½œæµ](mdc:design_pattern/workflow.html)ã€[RAG](mdc:design_pattern/rag.html)ç­‰ã€‚  
- **æ™ºèƒ½ä½“ç¼–ç **ï¼šè¶³å¤Ÿç›´è§‚ï¼ŒAIæ™ºèƒ½ä½“å¯å¸®åŠ©äººç±»æž„å»ºå¤æ‚çš„å¤§è¯­è¨€æ¨¡åž‹åº”ç”¨ã€‚

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/meme.jpg?raw=true" width="400"/>
</div>

## æ ¸å¿ƒæŠ½è±¡

æˆ‘ä»¬å°†å¤§è¯­è¨€æ¨¡åž‹å·¥ä½œæµå»ºæ¨¡ä¸º**å›¾ + å…±äº«å­˜å‚¨**ï¼š

- [èŠ‚ç‚¹](mdc:core_abstraction/node.md)å¤„ç†ç®€å•ï¼ˆå¤§è¯­è¨€æ¨¡åž‹ï¼‰ä»»åŠ¡ã€‚
- [æµç¨‹](mdc:core_abstraction/flow.md)é€šè¿‡**åŠ¨ä½œ**ï¼ˆå¸¦æ ‡ç­¾çš„è¾¹ï¼‰è¿žæŽ¥èŠ‚ç‚¹ã€‚
- [å…±äº«å­˜å‚¨](mdc:core_abstraction/communication.md)å®žçŽ°æµç¨‹å†…èŠ‚ç‚¹é—´çš„é€šä¿¡ã€‚
- [æ‰¹é‡](mdc:core_abstraction/batch.md)èŠ‚ç‚¹/æµç¨‹æ”¯æŒæ•°æ®å¯†é›†åž‹ä»»åŠ¡ã€‚
- [å¼‚æ­¥](mdc:core_abstraction/async.md)èŠ‚ç‚¹/æµç¨‹æ”¯æŒç­‰å¾…å¼‚æ­¥ä»»åŠ¡ã€‚
- [(é«˜çº§)å¹¶è¡Œ](mdc:core_abstraction/parallel.md)èŠ‚ç‚¹/æµç¨‹å¤„ç†I/Oå¯†é›†åž‹ä»»åŠ¡ã€‚

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/abstraction.png" width="500"/>
</div>

## è®¾è®¡æ¨¡å¼

åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæ˜“äºŽå®žçŽ°æµè¡Œçš„è®¾è®¡æ¨¡å¼ï¼š

- [æ™ºèƒ½ä½“](mdc:design_pattern/agent.md)è‡ªä¸»å†³ç­–ã€‚
- [å·¥ä½œæµ](mdc:design_pattern/workflow.md)å°†å¤šä¸ªä»»åŠ¡é“¾æŽ¥ä¸ºæµæ°´çº¿ã€‚
- [RAG](mdc:design_pattern/rag.md)é›†æˆæ•°æ®æ£€ç´¢ä¸Žç”Ÿæˆã€‚
- [Map Reduce](mdc:design_pattern/mapreduce.md)å°†æ•°æ®ä»»åŠ¡æ‹†åˆ†ä¸ºæ˜ å°„å’Œå½’çº¦æ­¥éª¤ã€‚
- [ç»“æž„åŒ–è¾“å‡º](mdc:design_pattern/structure.md)ä¸€è‡´åœ°æ ¼å¼åŒ–è¾“å‡ºã€‚
- [(é«˜çº§)å¤šæ™ºèƒ½ä½“](mdc:design_pattern/multi_agent.md)åè°ƒå¤šä¸ªæ™ºèƒ½ä½“ã€‚

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/design.png" width="500"/>
</div>

## å·¥å…·å‡½æ•°

æˆ‘ä»¬**ä¸**æä¾›å†…ç½®å·¥å…·ï¼Œè€Œæ˜¯æä¾›**ç¤ºä¾‹**â€”â€”è¯·**è‡ªè¡Œå®žçŽ°**ï¼š

- [å¤§è¯­è¨€æ¨¡åž‹å°è£…](mdc:utility_function/llm.md)
- [å¯è§†åŒ–ä¸Žè°ƒè¯•](mdc:utility_function/viz.md)
- [ç½‘é¡µæœç´¢](mdc:utility_function/websearch.md)
- [åˆ†å—](mdc:utility_function/chunking.md)
- [åµŒå…¥](mdc:utility_function/embedding.md)
- [å‘é‡æ•°æ®åº“](mdc:utility_function/vector.md)
- [æ–‡æœ¬è½¬è¯­éŸ³](mdc:utility_function/text_to_speech.md)

**ä¸ºä»€ä¹ˆä¸å†…ç½®ï¼Ÿ**ï¼šæˆ‘è®¤ä¸ºåœ¨é€šç”¨æ¡†æž¶ä¸­ç¡¬ç¼–ç åŽ‚å•†ç‰¹å®šAPIæ˜¯**ä¸è‰¯å®žè·µ**ï¼š
- **APIæ˜“å˜**ï¼šé¢‘ç¹å˜æ›´å¯¼è‡´ç¡¬ç¼–ç APIçš„ç»´æŠ¤æˆæœ¬é«˜ã€‚
- **çµæ´»æ€§**ï¼šä½ å¯èƒ½æƒ³åˆ‡æ¢åŽ‚å•†ã€ä½¿ç”¨å¾®è°ƒæ¨¡åž‹æˆ–æœ¬åœ°è¿è¡Œã€‚
- **ä¼˜åŒ–**ï¼šæ— åŽ‚å•†é”å®šæ—¶ï¼Œæç¤ºç¼“å­˜ã€æ‰¹é‡å¤„ç†å’Œæµå¼ä¼ è¾“æ›´ç®€å•ã€‚

## å‡†å¤‡å¥½æž„å»ºä½ çš„åº”ç”¨äº†å—ï¼Ÿ

æŸ¥çœ‹[æ™ºèƒ½ä½“ç¼–ç æŒ‡å—](mdc:guide.md)ï¼Œè¿™æ˜¯ä½¿ç”¨Pocket Flowå¼€å‘å¤§è¯­è¨€æ¨¡åž‹é¡¹ç›®çš„æœ€å¿«æ–¹å¼ï¼

================================================
File: docs/core_abstraction/async.md
================================================
---
layout: default
title: "(é«˜çº§) å¼‚æ­¥"
parent: "æ ¸å¿ƒæŠ½è±¡"
nav_order: 5
---

# (é«˜çº§) å¼‚æ­¥

**å¼‚æ­¥**èŠ‚ç‚¹å®žçŽ°`prep_async()`ã€`exec_async()`ã€`exec_fallback_async()`å’Œ/æˆ–`post_async()`ã€‚è¿™åœ¨ä»¥ä¸‹åœºæ™¯å¾ˆæœ‰ç”¨ï¼š

1. **prep_async()**ï¼šç”¨äºŽä»¥I/Oå‹å¥½çš„æ–¹å¼**èŽ·å–/è¯»å–æ•°æ®ï¼ˆæ–‡ä»¶ã€APIã€æ•°æ®åº“ï¼‰**ã€‚
2. **exec_async()**ï¼šé€šå¸¸ç”¨äºŽå¼‚æ­¥å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨ã€‚
3. **post_async()**ï¼šç”¨äºŽ**ç­‰å¾…ç”¨æˆ·åé¦ˆ**ã€**å¤šæ™ºèƒ½ä½“åè°ƒ**æˆ–`exec_async()`ä¹‹åŽçš„ä»»ä½•é¢å¤–å¼‚æ­¥æ­¥éª¤ã€‚

**æ³¨æ„**ï¼š`AsyncNode`å¿…é¡»åŒ…è£…åœ¨`AsyncFlow`ä¸­ã€‚`AsyncFlow`ä¹Ÿå¯åŒ…å«å¸¸è§„ï¼ˆåŒæ­¥ï¼‰èŠ‚ç‚¹ã€‚

### ç¤ºä¾‹

```python
class SummarizeThenVerify(AsyncNode):
    async def prep_async(self, shared):
        # ç¤ºä¾‹ï¼šå¼‚æ­¥è¯»å–æ–‡ä»¶
        doc_text = await read_file_async(shared["doc_path"])
        return doc_text

    async def exec_async(self, prep_res):
        # ç¤ºä¾‹ï¼šå¼‚æ­¥å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨
        summary = await call_llm_async(f"æ€»ç»“ï¼š{prep_res}")
        return summary

    async def post_async(self, shared, prep_res, exec_res):
        # ç¤ºä¾‹ï¼šç­‰å¾…ç”¨æˆ·åé¦ˆ
        decision = await gather_user_feedback(exec_res)
        if decision == "approve":
            shared["summary"] = exec_res
            return "approve"
        return "deny"

summarize_node = SummarizeThenVerify()
final_node = Finalize()

# å®šä¹‰è¿‡æ¸¡
summarize_node - "approve" >> final_node
summarize_node - "deny"    >> summarize_node  # é‡è¯•

flow = AsyncFlow(start=summarize_node)

async def main():
    shared = {"doc_path": "document.txt"}
    await flow.run_async(shared)
    print("æœ€ç»ˆæ€»ç»“ï¼š", shared.get("summary"))

asyncio.run(main())
```

================================================
File: docs/core_abstraction/batch.md
================================================
---
layout: default
title: "æ‰¹é‡"
parent: "æ ¸å¿ƒæŠ½è±¡"
nav_order: 4
---

# æ‰¹é‡

**æ‰¹é‡**ä¾¿äºŽåœ¨ä¸€ä¸ªèŠ‚ç‚¹ä¸­å¤„ç†å¤§é‡è¾“å…¥æˆ–**é‡å¤è¿è¡Œ**æµç¨‹ã€‚ç¤ºä¾‹ç”¨ä¾‹ï¼š
- **åŸºäºŽå—**çš„å¤„ç†ï¼ˆå¦‚æ‹†åˆ†å¤§æ–‡æœ¬ï¼‰ã€‚
- å¯¹è¾“å…¥é¡¹åˆ—è¡¨ï¼ˆå¦‚ç”¨æˆ·æŸ¥è¯¢ã€æ–‡ä»¶ã€URLï¼‰çš„**è¿­ä»£**å¤„ç†ã€‚

## 1. æ‰¹é‡èŠ‚ç‚¹ï¼ˆBatchNodeï¼‰

**æ‰¹é‡èŠ‚ç‚¹**æ‰©å±•`Node`ï¼Œä½†ä¿®æ”¹äº†`prep()`å’Œ`exec()`ï¼š

- **`prep(shared)`**ï¼šè¿”å›ž**å¯è¿­ä»£å¯¹è±¡**ï¼ˆå¦‚åˆ—è¡¨ã€ç”Ÿæˆå™¨ï¼‰ã€‚
- **`exec(item)`**ï¼šå¯¹å¯è¿­ä»£å¯¹è±¡ä¸­çš„æ¯ä¸ªé¡¹**è°ƒç”¨ä¸€æ¬¡**ã€‚
- **`post(shared, prep_res, exec_res_list)`**ï¼šæ‰€æœ‰é¡¹å¤„ç†å®ŒæˆåŽï¼ŒæŽ¥æ”¶ç»“æžœ**åˆ—è¡¨**ï¼ˆ`exec_res_list`ï¼‰å¹¶è¿”å›ž**åŠ¨ä½œ**ã€‚


### ç¤ºä¾‹ï¼šæ€»ç»“å¤§æ–‡ä»¶

```python
class MapSummaries(BatchNode):
    def prep(self, shared):
        # å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤§æ–‡ä»¶ï¼›å°†å…¶åˆ†å—
        content = shared["data"]
        chunk_size = 10000
        chunks = [content[i:i+chunk_size] for i in range(0, len(content), chunk_size)]
        return chunks

    def exec(self, chunk):
        prompt = f"ç”¨10ä¸ªè¯æ€»ç»“æ­¤å—ï¼š{chunk}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res_list):
        combined = "\n".join(exec_res_list)
        shared["summary"] = combined
        return "default"

map_summaries = MapSummaries()
flow = Flow(start=map_summaries)
flow.run(shared)
```

---

## 2. æ‰¹é‡æµç¨‹ï¼ˆBatchFlowï¼‰

**æ‰¹é‡æµç¨‹**å¤šæ¬¡è¿è¡Œä¸€ä¸ª**æµç¨‹**ï¼Œæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„`params`ã€‚å¯è§†ä¸ºä¸ºæ¯ä¸ªå‚æ•°é›†é‡æ”¾æµç¨‹çš„å¾ªçŽ¯ã€‚

### ç¤ºä¾‹ï¼šæ€»ç»“å¤šä¸ªæ–‡ä»¶

```python
class SummarizeAllFiles(BatchFlow):
    def prep(self, shared):
        # è¿”å›žå‚æ•°å­—å…¸åˆ—è¡¨ï¼ˆæ¯ä¸ªæ–‡ä»¶ä¸€ä¸ªï¼‰
        filenames = list(shared["data"].keys())  # å¦‚ ["file1.txt", "file2.txt", ...]
        return [{"filename": fn} for fn in filenames]

# å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªå•æ–‡ä»¶æµç¨‹ï¼ˆå¦‚ load_file >> summarize >> reduceï¼‰ï¼š
summarize_file = SummarizeFile(start=load_file)

# å°†è¯¥æµç¨‹åŒ…è£…åˆ°æ‰¹é‡æµç¨‹ä¸­ï¼š
summarize_all_files = SummarizeAllFiles(start=summarize_file)
summarize_all_files.run(shared)
```

### å†…éƒ¨åŽŸç†
1. `prep(shared)`è¿”å›žå‚æ•°å­—å…¸åˆ—è¡¨â€”â€”å¦‚`[{filename: "file1.txt"}, {filename: "file2.txt"}, ...]`ã€‚
2. **æ‰¹é‡æµç¨‹**éåŽ†æ¯ä¸ªå­—å…¸ã€‚å¯¹äºŽæ¯ä¸ªå­—å…¸ï¼š
   - å°†å…¶ä¸Žæ‰¹é‡æµç¨‹è‡ªèº«çš„`params`åˆå¹¶ã€‚
   - ä½¿ç”¨åˆå¹¶ç»“æžœè°ƒç”¨`flow.run(shared)`ã€‚
3. è¿™æ„å‘³ç€å­æµç¨‹ä¼š**é‡å¤è¿è¡Œ**ï¼Œæ¯ä¸ªå‚æ•°å­—å…¸è¿è¡Œä¸€æ¬¡ã€‚

---

## 3. åµŒå¥—æˆ–å¤šçº§æ‰¹é‡

å¯ä»¥åœ¨å¦ä¸€ä¸ª**æ‰¹é‡æµç¨‹**ä¸­åµŒå¥—ä¸€ä¸ª**æ‰¹é‡æµç¨‹**ã€‚ä¾‹å¦‚ï¼š
- **å¤–éƒ¨**æ‰¹é‡ï¼šè¿”å›žç›®å½•å‚æ•°å­—å…¸åˆ—è¡¨ï¼ˆå¦‚`{"directory": "/pathA"}`ï¼Œ`{"directory": "/pathB"}`ï¼Œ...ï¼‰ã€‚
- **å†…éƒ¨**æ‰¹é‡ï¼šè¿”å›žæ¯ä¸ªæ–‡ä»¶çš„å‚æ•°å­—å…¸åˆ—è¡¨ã€‚

åœ¨æ¯ä¸ªçº§åˆ«ï¼Œ**æ‰¹é‡æµç¨‹**å°†è‡ªèº«çš„å‚æ•°å­—å…¸ä¸Žçˆ¶çº§çš„åˆå¹¶ã€‚åˆ°è¾¾**æœ€å†…å±‚**èŠ‚ç‚¹æ—¶ï¼Œæœ€ç»ˆçš„`params`æ˜¯é“¾ä¸­**æ‰€æœ‰**çˆ¶çº§åˆå¹¶çš„ç»“æžœã€‚è¿™æ ·ï¼ŒåµŒå¥—ç»“æž„å¯ä»¥åŒæ—¶è·Ÿè¸ªæ•´ä¸ªä¸Šä¸‹æ–‡ï¼ˆå¦‚ç›®å½•+æ–‡ä»¶åï¼‰ã€‚

```python

class FileBatchFlow(BatchFlow):
    def prep(self, shared):
        directory = self.params["directory"]
        # å¦‚ files = ["file1.txt", "file2.txt", ...]
        files = [f for f in os.listdir(directory) if f.endswith(".txt")]
        return [{"filename": f} for f in files]

class DirectoryBatchFlow(BatchFlow):
    def prep(self, shared):
        directories = [ "/path/to/dirA", "/path/to/dirB"]
        return [{"directory": d} for d in directories]

# MapSummariesçš„å‚æ•°å¦‚ {"directory": "/path/to/dirA", "filename": "file1.txt"}
inner_flow = FileBatchFlow(start=MapSummaries())
outer_flow = DirectoryBatchFlow(start=inner_flow)
```

================================================
File: docs/core_abstraction/communication.md
================================================
---
layout: default
title: "é€šä¿¡"
parent: "æ ¸å¿ƒæŠ½è±¡"
nav_order: 3
---

# é€šä¿¡

èŠ‚ç‚¹å’Œæµç¨‹é€šè¿‡2ç§æ–¹å¼**é€šä¿¡**ï¼š

1. **å…±äº«å­˜å‚¨ï¼ˆå‡ ä¹Žæ‰€æœ‰æƒ…å†µï¼‰** 

   - æ‰€æœ‰èŠ‚ç‚¹å¯è¯»å–ï¼ˆ`prep()`ï¼‰å’Œå†™å…¥ï¼ˆ`post()`ï¼‰çš„å…¨å±€æ•°æ®ç»“æž„ï¼ˆé€šå¸¸æ˜¯å†…å­˜å­—å…¸ï¼‰ã€‚  
   - é€‚ç”¨äºŽæ•°æ®ç»“æžœã€å¤§å†…å®¹æˆ–å¤šä¸ªèŠ‚ç‚¹éœ€è¦çš„ä»»ä½•å†…å®¹ã€‚
   - ä½ éœ€è¦è®¾è®¡æ•°æ®ç»“æž„å¹¶æå‰å¡«å……ã€‚
     
   - > **å…³æ³¨ç‚¹åˆ†ç¦»**ï¼šå‡ ä¹Žæ‰€æœ‰æƒ…å†µéƒ½ä½¿ç”¨**å…±äº«å­˜å‚¨**ï¼Œå°†*æ•°æ®æ¨¡å¼*ä¸Ž*è®¡ç®—é€»è¾‘*åˆ†ç¦»ï¼è¿™ç§æ–¹æ³•æ—¢çµæ´»åˆæ˜“äºŽç®¡ç†ï¼Œå¯äº§ç”Ÿæ›´æ˜“ç»´æŠ¤çš„ä»£ç ã€‚`Params`æ›´å¤šæ˜¯[æ‰¹é‡](mdc:batch.md)çš„è¯­æ³•ç³–ã€‚
     {: .best-practice }

2. **å‚æ•°ï¼ˆä»…ç”¨äºŽ[æ‰¹é‡](mdc:batch.md)ï¼‰** 
   - æ¯ä¸ªèŠ‚ç‚¹æœ‰ä¸€ä¸ªæœ¬åœ°ã€ä¸´æ—¶çš„`params`å­—å…¸ï¼Œç”±**çˆ¶æµç¨‹**ä¼ å…¥ï¼Œç”¨ä½œä»»åŠ¡æ ‡è¯†ç¬¦ã€‚å‚æ•°é”®å’Œå€¼åº”**ä¸å¯å˜**ã€‚
   - é€‚ç”¨äºŽæ‰¹é‡æ¨¡å¼ä¸­çš„æ ‡è¯†ç¬¦ï¼Œå¦‚æ–‡ä»¶åæˆ–æ•°å­—IDã€‚

å¦‚æžœä½ äº†è§£å†…å­˜ç®¡ç†ï¼Œå¯ä»¥å°†**å…±äº«å­˜å‚¨**è§†ä¸º**å †**ï¼ˆæ‰€æœ‰å‡½æ•°è°ƒç”¨å…±äº«ï¼‰ï¼Œå°†**å‚æ•°**è§†ä¸º**æ ˆ**ï¼ˆç”±è°ƒç”¨è€…åˆ†é…ï¼‰ã€‚

---

## 1. å…±äº«å­˜å‚¨

### æ¦‚è¿°

å…±äº«å­˜å‚¨é€šå¸¸æ˜¯å†…å­˜å­—å…¸ï¼Œå¦‚ï¼š
```python
shared = {"data": {}, "summary": {}, "config": {...}, ...}
```

å®ƒè¿˜å¯åŒ…å«æœ¬åœ°æ–‡ä»¶å¥æŸ„ã€æ•°æ®åº“è¿žæŽ¥æˆ–ç”¨äºŽæŒä¹…åŒ–çš„ç»„åˆã€‚å»ºè®®æ ¹æ®åº”ç”¨éœ€æ±‚å…ˆç¡®å®šæ•°æ®ç»“æž„æˆ–æ•°æ®åº“æ¨¡å¼ã€‚

### ç¤ºä¾‹

```python
class LoadData(Node):
    def post(self, shared, prep_res, exec_res):
        # å†™å…¥æ•°æ®åˆ°å…±äº«å­˜å‚¨
        shared["data"] = "ä¸€äº›æ–‡æœ¬å†…å®¹"
        return None

class Summarize(Node):
    def prep(self, shared):
        # ä»Žå…±äº«å­˜å‚¨è¯»å–æ•°æ®
        return shared["data"]

    def exec(self, prep_res):
        # è°ƒç”¨å¤§è¯­è¨€æ¨¡åž‹æ€»ç»“
        prompt = f"æ€»ç»“ï¼š{prep_res}"
        summary = call_llm(prompt)
        return summary

    def post(self, shared, prep_res, exec_res):
        # å°†æ€»ç»“å†™å…¥å…±äº«å­˜å‚¨
        shared["summary"] = exec_res
        return "default"

load_data = LoadData()
summarize = Summarize()
load_data >> summarize
flow = Flow(start=load_data)

shared = {}
flow.run(shared)
```

è¿™é‡Œï¼š
- `LoadData`å†™å…¥`shared["data"]`ã€‚
- `Summarize`ä»Ž`shared["data"]`è¯»å–ï¼Œæ€»ç»“åŽå†™å…¥`shared["summary"]`ã€‚

---

## 2. å‚æ•°

**å‚æ•°**å…è®¸ä½ å­˜å‚¨ä¸éœ€è¦æ”¾åœ¨å…±äº«å­˜å‚¨ä¸­çš„*æ¯ä¸ªèŠ‚ç‚¹*æˆ–*æ¯ä¸ªæµç¨‹*çš„é…ç½®ã€‚å®ƒä»¬ï¼š
- åœ¨èŠ‚ç‚¹è¿è¡Œå‘¨æœŸå†…**ä¸å¯å˜**ï¼ˆå³`prep->exec->post`è¿‡ç¨‹ä¸­ä¸å˜ï¼‰ã€‚
- é€šè¿‡`set_params()`**è®¾ç½®**ã€‚
- æ¯æ¬¡çˆ¶æµç¨‹è°ƒç”¨æ—¶**æ¸…é™¤**å¹¶æ›´æ–°ã€‚

> åªè®¾ç½®æœ€ä¸Šå±‚æµç¨‹çš„å‚æ•°ï¼Œå› ä¸ºå…¶ä»–å‚æ•°ä¼šè¢«çˆ¶æµç¨‹è¦†ç›–ã€‚
> 
> å¦‚æžœéœ€è¦è®¾ç½®å­èŠ‚ç‚¹å‚æ•°ï¼Œå‚è§[æ‰¹é‡](mdc:batch.md)ã€‚
{: .warning }

é€šå¸¸ï¼Œ**å‚æ•°**æ˜¯æ ‡è¯†ç¬¦ï¼ˆå¦‚æ–‡ä»¶åã€é¡µç ï¼‰ã€‚ç”¨å®ƒä»¬æ¥èŽ·å–åˆ†é…çš„ä»»åŠ¡æˆ–å†™å…¥å…±äº«å­˜å‚¨çš„ç‰¹å®šéƒ¨åˆ†ã€‚

### ç¤ºä¾‹

```python
# 1) åˆ›å»ºä½¿ç”¨å‚æ•°çš„èŠ‚ç‚¹
class SummarizeFile(Node):
    def prep(self, shared):
        # è®¿é—®èŠ‚ç‚¹çš„å‚æ•°
        filename = self.params["filename"]
        return shared["data"].get(filename, "")

    def exec(self, prep_res):
        prompt = f"æ€»ç»“ï¼š{prep_res}"
        return call_llm(prompt)

    def post(self, shared, prep_res, exec_res):
        filename = self.params["filename"]
        shared["summary"][filename] = exec_res
        return "default"

# 2) è®¾ç½®å‚æ•°
node = SummarizeFile()

# 3) ç›´æŽ¥è®¾ç½®èŠ‚ç‚¹å‚æ•°ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰
node.set_params({"filename": "doc1.txt"})
node.run(shared)

# 4) åˆ›å»ºæµç¨‹
flow = Flow(start=node)

# 5) è®¾ç½®æµç¨‹å‚æ•°ï¼ˆè¦†ç›–èŠ‚ç‚¹å‚æ•°ï¼‰
flow.set_params({"filename": "doc2.txt"})
flow.run(shared)  # èŠ‚ç‚¹æ€»ç»“doc2ï¼Œè€Œéždoc1
```

================================================
File: docs/core_abstraction/flow.md
================================================
---
layout: default
title: "æµç¨‹"
parent: "æ ¸å¿ƒæŠ½è±¡"
nav_order: 2
---

# æµç¨‹

**æµç¨‹**ç¼–æŽ’èŠ‚ç‚¹å›¾ã€‚ä½ å¯ä»¥æŒ‰é¡ºåºé“¾æŽ¥èŠ‚ç‚¹ï¼Œæˆ–æ ¹æ®æ¯ä¸ªèŠ‚ç‚¹`post()`è¿”å›žçš„**åŠ¨ä½œ**åˆ›å»ºåˆ†æ”¯ã€‚

## 1. åŸºäºŽåŠ¨ä½œçš„è¿‡æ¸¡

æ¯ä¸ªèŠ‚ç‚¹çš„`post()`è¿”å›ž**åŠ¨ä½œ**å­—ç¬¦ä¸²ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå¦‚æžœ`post()`ä¸è¿”å›žä»»ä½•å†…å®¹ï¼Œè§†ä¸º`"default"`ã€‚

ç”¨ä»¥ä¸‹è¯­æ³•å®šä¹‰è¿‡æ¸¡ï¼š

1. **åŸºæœ¬é»˜è®¤è¿‡æ¸¡**ï¼š`node_a >> node_b`
  è¡¨ç¤ºå¦‚æžœ`node_a.post()`è¿”å›ž`"default"`ï¼Œåˆ™è¿›å…¥`node_b`ã€‚
  ï¼ˆç­‰åŒäºŽ`node_a - "default" >> node_b`ï¼‰

2. **å‘½ååŠ¨ä½œè¿‡æ¸¡**ï¼š`node_a - "action_name" >> node_b`
  è¡¨ç¤ºå¦‚æžœ`node_a.post()`è¿”å›ž`"action_name"`ï¼Œåˆ™è¿›å…¥`node_b`ã€‚

å¯ä»¥åˆ›å»ºå¾ªçŽ¯ã€åˆ†æ”¯æˆ–å¤šæ­¥éª¤æµç¨‹ã€‚

## 2. åˆ›å»ºæµç¨‹

**æµç¨‹**ä»Ž**èµ·å§‹**èŠ‚ç‚¹å¼€å§‹ã€‚è°ƒç”¨`Flow(start=some_node)`æŒ‡å®šå…¥å£ç‚¹ã€‚è°ƒç”¨`flow.run(shared)`æ—¶ï¼Œå®ƒæ‰§è¡Œèµ·å§‹èŠ‚ç‚¹ï¼ŒæŸ¥çœ‹å…¶`post()`è¿”å›žçš„åŠ¨ä½œï¼Œéµå¾ªè¿‡æ¸¡ï¼Œç›´åˆ°æ²¡æœ‰ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ã€‚

### ç¤ºä¾‹ï¼šç®€å•åºåˆ—

ä»¥ä¸‹æ˜¯ä¸¤ä¸ªèŠ‚ç‚¹é“¾å¼è¿žæŽ¥çš„æœ€å°æµç¨‹ï¼š

```python
node_a >> node_b
flow = Flow(start=node_a)
flow.run(shared)
```

- è¿è¡Œæµç¨‹æ—¶ï¼Œæ‰§è¡Œ`node_a`ã€‚
- å‡è®¾`node_a.post()`è¿”å›ž`"default"`ã€‚
- æµç¨‹çœ‹åˆ°`"default"`åŠ¨ä½œé“¾æŽ¥åˆ°`node_b`ï¼Œç„¶åŽè¿è¡Œ`node_b`ã€‚
- `node_b.post()`è¿”å›ž`"default"`ï¼Œä½†æœªå®šä¹‰`node_b >> something_else`ã€‚å› æ­¤æµç¨‹åœ¨æ­¤ç»“æŸã€‚

### ç¤ºä¾‹ï¼šåˆ†æ”¯ä¸Žå¾ªçŽ¯

ä»¥ä¸‹æ˜¯ç®€å•çš„è´¹ç”¨å®¡æ‰¹æµç¨‹ï¼Œå±•ç¤ºåˆ†æ”¯å’Œå¾ªçŽ¯ã€‚`ReviewExpense`èŠ‚ç‚¹å¯è¿”å›žä¸‰ä¸ªå¯èƒ½çš„åŠ¨ä½œï¼š

- `"approved"`ï¼šè´¹ç”¨èŽ·æ‰¹ï¼Œè¿›å…¥æ”¯ä»˜å¤„ç†
- `"needs_revision"`ï¼šè´¹ç”¨éœ€è¦ä¿®æ”¹ï¼Œè¿”å›žä¿®è®¢
- `"rejected"`ï¼šè´¹ç”¨è¢«æ‹’ç»ï¼Œç»“æŸæµç¨‹

å¯ä»¥è¿™æ ·è¿žæŽ¥ï¼š

```python
# å®šä¹‰æµç¨‹è¿žæŽ¥
review - "approved" >> payment        # å¦‚èŽ·æ‰¹ï¼Œå¤„ç†æ”¯ä»˜
review - "needs_revision" >> revise   # å¦‚éœ€ä¿®æ”¹ï¼Œè¿›å…¥ä¿®è®¢
review - "rejected" >> finish         # å¦‚æ‹’ç»ï¼Œç»“æŸæµç¨‹

revise >> review   # ä¿®è®¢åŽï¼Œè¿”å›žå†æ¬¡å®¡æ ¸
payment >> finish  # æ”¯ä»˜åŽï¼Œç»“æŸæµç¨‹

flow = Flow(start=review)
```

æµç¨‹å¦‚ä¸‹ï¼š

1. å¦‚æžœ`review.post()`è¿”å›ž`"approved"`ï¼Œè´¹ç”¨è¿›å…¥`payment`èŠ‚ç‚¹
2. å¦‚æžœ`review.post()`è¿”å›ž`"needs_revision"`ï¼Œè¿›å…¥`revise`èŠ‚ç‚¹ï¼Œç„¶åŽå¾ªçŽ¯å›ž`review`
3. å¦‚æžœ`review.post()`è¿”å›ž`"rejected"`ï¼Œè¿›å…¥`finish`èŠ‚ç‚¹å¹¶åœæ­¢

```mermaid
flowchart TD
    review[å®¡æ ¸è´¹ç”¨] -->|approved| payment[å¤„ç†æ”¯ä»˜]
    review -->|needs_revision| revise[ä¿®è®¢æŠ¥å‘Š]
    review -->|rejected| finish[ç»“æŸæµç¨‹]

    revise --> review
    payment --> finish
```

### è¿è¡Œå•ä¸ªèŠ‚ç‚¹ vs è¿è¡Œæµç¨‹

- `node.run(shared)`ï¼šä»…è¿è¡Œè¯¥èŠ‚ç‚¹ï¼ˆè°ƒç”¨`prep->exec->post()`ï¼‰ï¼Œè¿”å›žåŠ¨ä½œã€‚
- `flow.run(shared)`ï¼šä»Žèµ·å§‹èŠ‚ç‚¹æ‰§è¡Œï¼Œæ ¹æ®åŠ¨ä½œè¿›å…¥ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç›´åˆ°æ— æ³•ç»§ç»­ã€‚

> `node.run(shared)`**ä¸ä¼š**è¿›å…¥åŽç»­èŠ‚ç‚¹ã€‚
> ä¸»è¦ç”¨äºŽè°ƒè¯•æˆ–æµ‹è¯•å•ä¸ªèŠ‚ç‚¹ã€‚
> 
> ç”Ÿäº§çŽ¯å¢ƒä¸­å§‹ç»ˆä½¿ç”¨`flow.run(...)`ï¼Œç¡®ä¿å®Œæ•´æµæ°´çº¿æ­£ç¡®è¿è¡Œã€‚
{: .warning }

## 3. åµŒå¥—æµç¨‹

**æµç¨‹**å¯ä»¥åƒèŠ‚ç‚¹ä¸€æ ·å·¥ä½œï¼Œæ”¯æŒå¼ºå¤§çš„ç»„åˆæ¨¡å¼ã€‚è¿™æ„å‘³ç€ä½ å¯ä»¥ï¼š

1. åœ¨å¦ä¸€ä¸ªæµç¨‹çš„è¿‡æ¸¡ä¸­ä½¿ç”¨æµç¨‹ä½œä¸ºèŠ‚ç‚¹ã€‚
2. å°†å¤šä¸ªå°æµç¨‹ç»„åˆæˆä¸€ä¸ªå¤§æµç¨‹ä»¥ä¾¿é‡ç”¨ã€‚
3. èŠ‚ç‚¹`params`å°†æ˜¯**æ‰€æœ‰**çˆ¶çº§`params`çš„åˆå¹¶ã€‚

### æµç¨‹çš„èŠ‚ç‚¹æ–¹æ³•

**æµç¨‹**ä¹Ÿæ˜¯**èŠ‚ç‚¹**ï¼Œå› æ­¤ä¼šè¿è¡Œ`prep()`å’Œ`post()`ã€‚ä½†ï¼š

- å®ƒ**ä¸ä¼š**è¿è¡Œ`exec()`ï¼Œå› ä¸ºå…¶ä¸»è¦é€»è¾‘æ˜¯ç¼–æŽ’èŠ‚ç‚¹ã€‚
- `post()`å§‹ç»ˆæŽ¥æ”¶`None`ä½œä¸º`exec_res`ï¼Œåº”ä»Žå…±äº«å­˜å‚¨èŽ·å–æµç¨‹æ‰§è¡Œç»“æžœã€‚

### åŸºæœ¬æµç¨‹åµŒå¥—

ä»¥ä¸‹æ˜¯å°†æµç¨‹è¿žæŽ¥åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹çš„æ–¹å¼ï¼š

```python
# åˆ›å»ºå­æµç¨‹
node_a >> node_b
subflow = Flow(start=node_a)

# å°†å…¶è¿žæŽ¥åˆ°å¦ä¸€ä¸ªèŠ‚ç‚¹
subflow >> node_c

# åˆ›å»ºçˆ¶æµç¨‹
parent_flow = Flow(start=subflow)
```

`parent_flow.run()`æ‰§è¡Œæ—¶ï¼š
1. å¯åŠ¨`subflow`
2. `subflow`è¿è¡Œå…¶èŠ‚ç‚¹ï¼ˆ`node_a->node_b`ï¼‰
3. `subflow`å®ŒæˆåŽï¼Œç»§ç»­æ‰§è¡Œ`node_c`

### ç¤ºä¾‹ï¼šè®¢å•å¤„ç†æµæ°´çº¿

ä»¥ä¸‹æ˜¯å°†è®¢å•å¤„ç†æ‹†åˆ†ä¸ºåµŒå¥—æµç¨‹çš„å®žé™…ç¤ºä¾‹ï¼š

```python
# æ”¯ä»˜å¤„ç†å­æµç¨‹
validate_payment >> process_payment >> payment_confirmation
payment_flow = Flow(start=validate_payment)

# åº“å­˜å­æµç¨‹
check_stock >> reserve_items >> update_inventory
inventory_flow = Flow(start=check_stock)

# ç‰©æµå­æµç¨‹
create_label >> assign_carrier >> schedule_pickup
shipping_flow = Flow(start=create_label)

# å°†æµç¨‹è¿žæŽ¥æˆä¸»è®¢å•æµæ°´çº¿
payment_flow >> inventory_flow >> shipping_flow

# åˆ›å»ºä¸»æµç¨‹
order_pipeline = Flow(start=payment_flow)

# è¿è¡Œæ•´ä¸ªæµæ°´çº¿
order_pipeline.run(shared_data)
```

è¿™å®žçŽ°äº†æ¸…æ™°çš„å…³æ³¨ç‚¹åˆ†ç¦»ï¼ŒåŒæ—¶ä¿æŒæ˜Žç¡®çš„æ‰§è¡Œè·¯å¾„ï¼š

```mermaid
flowchart LR
    subgraph order_pipeline[è®¢å•æµæ°´çº¿]
        subgraph paymentFlow["æ”¯ä»˜æµç¨‹"]
            A[éªŒè¯æ”¯ä»˜] --> B[å¤„ç†æ”¯ä»˜] --> C[æ”¯ä»˜ç¡®è®¤]
        end

        subgraph inventoryFlow["åº“å­˜æµç¨‹"]
            D[æ£€æŸ¥åº“å­˜] --> E[é¢„ç•™å•†å“] --> F[æ›´æ–°åº“å­˜]
        end

        subgraph shippingFlow["ç‰©æµæµç¨‹"]
            G[åˆ›å»ºæ ‡ç­¾] --> H[åˆ†é…æ‰¿è¿äºº] --> I[å®‰æŽ’å–ä»¶]
        end

        paymentFlow --> inventoryFlow
        inventoryFlow --> shippingFlow
    end
```

================================================
File: docs/core_abstraction/node.md
================================================
---
layout: default
title: "èŠ‚ç‚¹"
parent: "æ ¸å¿ƒæŠ½è±¡"
nav_order: 1
---

# èŠ‚ç‚¹

**èŠ‚ç‚¹**æ˜¯æœ€å°çš„æž„å»ºå—ã€‚æ¯ä¸ªèŠ‚ç‚¹æœ‰3ä¸ªæ­¥éª¤`prep->exec->post`ï¼š

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/node.png?raw=true" width="400"/>
</div>

1. `prep(shared)`
   - ä»Ž`shared`å­˜å‚¨**è¯»å–å’Œé¢„å¤„ç†æ•°æ®**ã€‚
   - ç¤ºä¾‹ï¼š*æŸ¥è¯¢æ•°æ®åº“ã€è¯»å–æ–‡ä»¶æˆ–åºåˆ—åŒ–æ•°æ®ä¸ºå­—ç¬¦ä¸²*ã€‚
   - è¿”å›ž`prep_res`ï¼Œä¾›`exec()`å’Œ`post()`ä½¿ç”¨ã€‚

2. `exec(prep_res)`
   - **æ‰§è¡Œè®¡ç®—é€»è¾‘**ï¼Œå¯é€‰é‡è¯•å’Œé”™è¯¯å¤„ç†ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚
   - ç¤ºä¾‹ï¼š*(ä¸»è¦æ˜¯)å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨ã€è¿œç¨‹APIã€å·¥å…·ä½¿ç”¨*ã€‚
   - âš ï¸ ä»…ç”¨äºŽè®¡ç®—ï¼Œ**ä¸è¦**è®¿é—®`shared`ã€‚
   - âš ï¸ å¦‚æžœå¯ç”¨é‡è¯•ï¼Œç¡®ä¿å®žçŽ°ä¸ºå¹‚ç­‰çš„ã€‚
   - è¿”å›ž`exec_res`ï¼Œä¼ é€’ç»™`post()`ã€‚

3. `post(shared, prep_res, exec_res)`
   - **åŽå¤„ç†å¹¶å°†æ•°æ®å†™å›ž**`shared`ã€‚
   - ç¤ºä¾‹ï¼š*æ›´æ–°æ•°æ®åº“ã€æ›´æ”¹çŠ¶æ€ã€è®°å½•ç»“æžœ*ã€‚
   - é€šè¿‡è¿”å›ž**å­—ç¬¦ä¸²**å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œï¼ˆå¦‚æ— è¿”å›žï¼Œé»˜è®¤ä¸º`"default"`ï¼‰ã€‚

> **ä¸ºä»€ä¹ˆåˆ†3æ­¥ï¼Ÿ** ä¸ºäº†éµå¾ª*å…³æ³¨ç‚¹åˆ†ç¦»*åŽŸåˆ™ã€‚æ•°æ®å­˜å‚¨å’Œæ•°æ®å¤„ç†åˆ†å¼€æ“ä½œã€‚
>
> æ‰€æœ‰æ­¥éª¤éƒ½æ˜¯*å¯é€‰çš„*ã€‚ä¾‹å¦‚ï¼Œåªéœ€å¤„ç†æ•°æ®æ—¶ï¼Œå¯åªå®žçŽ°`prep`å’Œ`post`ã€‚
{: .note }

### å®¹é”™ä¸Žé‡è¯•

å®šä¹‰èŠ‚ç‚¹æ—¶ï¼Œå¯é€šè¿‡ä¸¤ä¸ªå‚æ•°**é‡è¯•**`exec()`ï¼ˆå¦‚æžœå…¶æŠ›å‡ºå¼‚å¸¸ï¼‰ï¼š

- `max_retries`ï¼ˆintï¼‰ï¼šè¿è¡Œ`exec()`çš„æœ€å¤§æ¬¡æ•°ã€‚é»˜è®¤æ˜¯`1`ï¼ˆ**ä¸**é‡è¯•ï¼‰ã€‚
- `wait`ï¼ˆintï¼‰ï¼šä¸‹æ¬¡é‡è¯•å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆ**ç§’**ï¼‰ã€‚é»˜è®¤`wait=0`ï¼ˆä¸ç­‰å¾…ï¼‰ã€‚
`wait`åœ¨é‡åˆ°å¤§è¯­è¨€æ¨¡åž‹æä¾›å•†çš„é€ŸçŽ‡é™åˆ¶æˆ–é…é¢é”™è¯¯æ—¶å¾ˆæœ‰ç”¨ï¼Œéœ€è¦é€€é¿ã€‚

```python 
my_node = SummarizeFile(max_retries=3, wait=10)
```

å½“`exec()`ä¸­å‘ç”Ÿå¼‚å¸¸æ—¶ï¼ŒèŠ‚ç‚¹ä¼šè‡ªåŠ¨é‡è¯•ï¼Œç›´åˆ°ï¼š

- æˆåŠŸï¼Œæˆ–
- èŠ‚ç‚¹å·²é‡è¯•`max_retries - 1`æ¬¡ï¼Œæœ€åŽä¸€æ¬¡å°è¯•å¤±è´¥ã€‚

å¯ä»Ž`self.cur_retry`èŽ·å–å½“å‰é‡è¯•æ¬¡æ•°ï¼ˆä»Ž0å¼€å§‹ï¼‰ã€‚

```python 
class RetryNode(Node):
    def exec(self, prep_res):
        print(f"å·²é‡è¯• {self.cur_retry} æ¬¡")
        raise Exception("å¤±è´¥")
```

### ä¼˜é›…é™çº§

ä¸ºåœ¨æ‰€æœ‰é‡è¯•åŽ**ä¼˜é›…å¤„ç†**å¼‚å¸¸ï¼ˆè€ŒéžæŠ›å‡ºï¼‰ï¼Œé‡å†™ï¼š

```python 
def exec_fallback(self, prep_res, exc):
    raise exc
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒåªä¼šé‡æ–°æŠ›å‡ºå¼‚å¸¸ã€‚ä½†ä½ å¯ä»¥è¿”å›žé™çº§ç»“æžœï¼Œä½œä¸ºä¼ é€’ç»™`post()`çš„`exec_res`ã€‚

### ç¤ºä¾‹ï¼šæ€»ç»“æ–‡ä»¶

```python 
class SummarizeFile(Node):
    def prep(self, shared):
        return shared["data"]

    def exec(self, prep_res):
        if not prep_res:
            return "æ–‡ä»¶å†…å®¹ä¸ºç©º"
        prompt = f"ç”¨10ä¸ªè¯æ€»ç»“æ­¤æ–‡æœ¬ï¼š{prep_res}"
        summary = call_llm(prompt)  # å¯èƒ½å¤±è´¥
        return summary

    def exec_fallback(self, prep_res, exc):
        # æä¾›ç®€å•é™çº§æ–¹æ¡ˆè€Œéžå´©æºƒ
        return "å¤„ç†è¯·æ±‚æ—¶å‡ºé”™ã€‚"

    def post(self, shared, prep_res, exec_res):
        shared["summary"] = exec_res
        # ä¸è¿”å›žå³é»˜è®¤"default"

summarize_node = SummarizeFile(max_retries=3)

# node.run() è°ƒç”¨ prep->exec->post
# å¦‚æžœexec()å¤±è´¥ï¼Œæœ€å¤šé‡è¯•3æ¬¡åŽè°ƒç”¨exec_fallback()
action_result = summarize_node.run(shared)

print("è¿”å›žçš„åŠ¨ä½œï¼š", action_result)  # "default"
print("å­˜å‚¨çš„æ€»ç»“ï¼š", shared["summary"])
```


================================================
File: docs/core_abstraction/parallel.md
================================================
---
layout: default
title: "(é«˜çº§) å¹¶è¡Œ"
parent: "æ ¸å¿ƒæŠ½è±¡"
nav_order: 6
---

# (é«˜çº§) å¹¶è¡Œ

**å¹¶è¡Œ**èŠ‚ç‚¹å’Œæµç¨‹å…è®¸**å¹¶å‘**è¿è¡Œå¤šä¸ª**å¼‚æ­¥**èŠ‚ç‚¹å’Œæµç¨‹â€”â€”ä¾‹å¦‚ï¼ŒåŒæ—¶æ€»ç»“å¤šä¸ªæ–‡æœ¬ã€‚è¿™å¯ä»¥é€šè¿‡é‡å I/Oå’Œè®¡ç®—æé«˜æ€§èƒ½ã€‚

> ç”±äºŽPythonçš„GILï¼Œå¹¶è¡ŒèŠ‚ç‚¹å’Œæµç¨‹ä¸èƒ½çœŸæ­£å¹¶è¡ŒåŒ–CPUå¯†é›†åž‹ä»»åŠ¡ï¼ˆå¦‚ç¹é‡çš„æ•°å€¼è®¡ç®—ï¼‰ã€‚ä½†å®ƒä»¬æ“…é•¿é‡å I/Oå¯†é›†åž‹å·¥ä½œâ€”â€”å¦‚å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨ã€æ•°æ®åº“æŸ¥è¯¢ã€APIè¯·æ±‚æˆ–æ–‡ä»¶I/Oã€‚
{: .warning }

> - **ç¡®ä¿ä»»åŠ¡ç‹¬ç«‹**ï¼šå¦‚æžœæ¯ä¸ªé¡¹ä¾èµ–å‰ä¸€é¡¹çš„è¾“å‡ºï¼Œ**ä¸è¦**å¹¶è¡ŒåŒ–ã€‚
> 
> - **æ³¨æ„é€ŸçŽ‡é™åˆ¶**ï¼šå¹¶è¡Œè°ƒç”¨å¯èƒ½**å¿«é€Ÿ**è§¦å‘å¤§è¯­è¨€æ¨¡åž‹æœåŠ¡çš„é€ŸçŽ‡é™åˆ¶ã€‚å¯èƒ½éœ€è¦**èŠ‚æµ**æœºåˆ¶ï¼ˆå¦‚ä¿¡å·é‡æˆ–ç¡çœ é—´éš”ï¼‰ã€‚
> 
> - **è€ƒè™‘å•èŠ‚ç‚¹æ‰¹é‡API**ï¼šä¸€äº›å¤§è¯­è¨€æ¨¡åž‹æä¾›**æ‰¹é‡æŽ¨ç†**APIï¼Œå¯åœ¨å•ä¸ªè°ƒç”¨ä¸­å‘é€å¤šä¸ªæç¤ºã€‚å®žçŽ°æ›´å¤æ‚ï¼Œä½†å¯èƒ½æ¯”å¯åŠ¨è®¸å¤šå¹¶è¡Œè¯·æ±‚æ›´é«˜æ•ˆï¼Œå¹¶å‡å°‘é€ŸçŽ‡é™åˆ¶é—®é¢˜ã€‚
{: .best-practice }

## å¼‚æ­¥å¹¶è¡Œæ‰¹é‡èŠ‚ç‚¹ï¼ˆAsyncParallelBatchNodeï¼‰

ç±»ä¼¼**AsyncBatchNode**ï¼Œä½†**å¹¶è¡Œ**è¿è¡Œ`exec_async()`ï¼š

```python
class ParallelSummaries(AsyncParallelBatchNode):
    async def prep_async(self, shared):
        # å¦‚å¤šä¸ªæ–‡æœ¬
        return shared["texts"]

    async def exec_async(self, text):
        prompt = f"æ€»ç»“ï¼š{text}"
        return await call_llm_async(prompt)

    async def post_async(self, shared, prep_res, exec_res_list):
        shared["summary"] = "\n\n".join(exec_res_list)
        return "default"

node = ParallelSummaries()
flow = AsyncFlow(start=node)
```

## å¼‚æ­¥å¹¶è¡Œæ‰¹é‡æµç¨‹ï¼ˆAsyncParallelBatchFlowï¼‰

**BatchFlow**çš„å¹¶è¡Œç‰ˆæœ¬ã€‚å­æµç¨‹çš„æ¯æ¬¡è¿­ä»£ä½¿ç”¨ä¸åŒå‚æ•°**å¹¶å‘**è¿è¡Œï¼š

```python
class SummarizeMultipleFiles(AsyncParallelBatchFlow):
    async def prep_async(self, shared):
        return [{"filename": f} for f in shared["files"]]

sub_flow = AsyncFlow(start=LoadAndSummarizeFile())
parallel_flow = SummarizeMultipleFiles(start=sub_flow)
await parallel_flow.run_async(shared)
```

================================================
File: docs/design_pattern/agent.md
================================================
---
layout: default
title: "æ™ºèƒ½ä½“"
parent: "è®¾è®¡æ¨¡å¼"
nav_order: 1
---

# æ™ºèƒ½ä½“

æ™ºèƒ½ä½“æ˜¯ä¸€ç§å¼ºå¤§çš„è®¾è®¡æ¨¡å¼ï¼Œå…¶ä¸­èŠ‚ç‚¹å¯ä»¥æ ¹æ®ä¸Šä¸‹æ–‡é‡‡å–åŠ¨æ€åŠ¨ä½œã€‚

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/agent.png?raw=true" width="350"/>
</div>

## ç”¨å›¾å®žçŽ°æ™ºèƒ½ä½“

1. **ä¸Šä¸‹æ–‡å’ŒåŠ¨ä½œ**ï¼šå®žçŽ°æä¾›ä¸Šä¸‹æ–‡å’Œæ‰§è¡ŒåŠ¨ä½œçš„èŠ‚ç‚¹ã€‚
2. **åˆ†æ”¯**ï¼šä½¿ç”¨åˆ†æ”¯å°†æ¯ä¸ªåŠ¨ä½œèŠ‚ç‚¹è¿žæŽ¥åˆ°æ™ºèƒ½ä½“èŠ‚ç‚¹ã€‚ä½¿ç”¨åŠ¨ä½œå…è®¸æ™ºèƒ½ä½“åœ¨èŠ‚ç‚¹é—´å¼•å¯¼[æµç¨‹](mdc:../core_abstraction/flow.md)â€”â€”å¯èƒ½åŒ…å«å¤šæ­¥éª¤å¾ªçŽ¯ã€‚
3. **æ™ºèƒ½ä½“èŠ‚ç‚¹**ï¼šæä¾›æç¤ºä»¥å†³å®šåŠ¨ä½œâ€”â€”ä¾‹å¦‚ï¼š

```python
f"""
### ä¸Šä¸‹æ–‡
ä»»åŠ¡ï¼š{task_description}
å…ˆå‰åŠ¨ä½œï¼š{previous_actions}
å½“å‰çŠ¶æ€ï¼š{current_state}

### åŠ¨ä½œç©ºé—´
[1] search
  æè¿°ï¼šä½¿ç”¨ç½‘é¡µæœç´¢èŽ·å–ç»“æžœ
  å‚æ•°ï¼š
    - query (str)ï¼šæœç´¢å†…å®¹

[2] answer
  æè¿°ï¼šåŸºäºŽç»“æžœå¾—å‡ºç»“è®º
  å‚æ•°ï¼š
    - result (str)ï¼šæä¾›çš„æœ€ç»ˆç­”æ¡ˆ

### ä¸‹ä¸€æ­¥åŠ¨ä½œ
æ ¹æ®å½“å‰ä¸Šä¸‹æ–‡å’Œå¯ç”¨åŠ¨ä½œç©ºé—´å†³å®šä¸‹ä¸€æ­¥ã€‚
æŒ‰ä»¥ä¸‹æ ¼å¼è¿”å›žï¼š

```yaml
thinking: |
    <ä½ çš„åˆ†æ­¥æŽ¨ç†è¿‡ç¨‹>
action: <action_name>
parameters:
    <parameter_name>: <parameter_value>
```"""
```

æž„å»º**é«˜æ€§èƒ½**å’Œ**å¯é **æ™ºèƒ½ä½“çš„æ ¸å¿ƒåœ¨äºŽï¼š

1. **ä¸Šä¸‹æ–‡ç®¡ç†**ï¼šæä¾›*ç›¸å…³ã€æœ€å°åŒ–çš„ä¸Šä¸‹æ–‡*ã€‚ä¾‹å¦‚ï¼Œä¸Žå…¶åŒ…å«æ•´ä¸ªèŠå¤©åŽ†å²ï¼Œä¸å¦‚é€šè¿‡[RAG](mdc:rag.md)æ£€ç´¢æœ€ç›¸å…³çš„å†…å®¹ã€‚å³ä½¿æœ‰æ›´å¤§çš„ä¸Šä¸‹æ–‡çª—å£ï¼Œå¤§è¯­è¨€æ¨¡åž‹ä»å¯èƒ½å—["ä¸­é—´é—å¿˜"](mdc:https:/arxiv.org/abs/2307.03172)å½±å“ï¼Œå¿½ç•¥æç¤ºä¸­é—´çš„å†…å®¹ã€‚

2. **åŠ¨ä½œç©ºé—´**ï¼šæä¾›*ç»“æž„æ¸…æ™°ä¸”æ˜Žç¡®*çš„åŠ¨ä½œé›†â€”â€”é¿å…é‡å ï¼Œå¦‚å•ç‹¬çš„`read_databases`æˆ–`read_csvs`ã€‚ç›¸åï¼Œå°†CSVå¯¼å…¥æ•°æ®åº“ã€‚

## è‰¯å¥½åŠ¨ä½œè®¾è®¡ç¤ºä¾‹

- **å¢žé‡å¼**ï¼šä»¥å¯ç®¡ç†çš„å—ï¼ˆ500è¡Œæˆ–1é¡µï¼‰æä¾›å†…å®¹ï¼Œè€Œéžä¸€æ¬¡æ€§å…¨éƒ¨æä¾›ã€‚

- **æ¦‚è§ˆ-æ·±å…¥**ï¼šå…ˆæä¾›é«˜å±‚ç»“æž„ï¼ˆç›®å½•ã€æ‘˜è¦ï¼‰ï¼Œå†å…è®¸æ·±å…¥ç»†èŠ‚ï¼ˆåŽŸå§‹æ–‡æœ¬ï¼‰ã€‚

- **å‚æ•°åŒ–/å¯ç¼–ç¨‹**ï¼šä¸ä½¿ç”¨å›ºå®šåŠ¨ä½œï¼Œæ”¯æŒå‚æ•°åŒ–ï¼ˆè¦é€‰æ‹©çš„åˆ—ï¼‰æˆ–å¯ç¼–ç¨‹ï¼ˆSQLæŸ¥è¯¢ï¼‰åŠ¨ä½œï¼Œä¾‹å¦‚è¯»å–CSVæ–‡ä»¶ã€‚

- **å›žæº¯**ï¼šå…è®¸æ™ºèƒ½ä½“æ’¤é”€ä¸Šä¸€æ­¥ï¼Œè€Œéžå®Œå…¨é‡å¯ï¼Œåœ¨é‡åˆ°é”™è¯¯æˆ–æ­»èƒ¡åŒæ—¶ä¿ç•™è¿›åº¦ã€‚

## ç¤ºä¾‹ï¼šæœç´¢æ™ºèƒ½ä½“

è¯¥æ™ºèƒ½ä½“ï¼š
1. å†³å®šæœç´¢è¿˜æ˜¯å›žç­”
2. å¦‚æžœæœç´¢ï¼Œå¾ªçŽ¯å›žæ¥å†³å®šæ˜¯å¦éœ€è¦æ›´å¤šæœç´¢
3. æ”¶é›†è¶³å¤Ÿä¸Šä¸‹æ–‡åŽå›žç­”

```python
class DecideAction(Node):
    def prep(self, shared):
        context = shared.get("context", "æ— å…ˆå‰æœç´¢")
        query = shared["query"]
        return query, context
        
    def exec(self, inputs):
        query, context = inputs
        prompt = f"""
ç»™å®šè¾“å…¥ï¼š{query}
å…ˆå‰æœç´¢ç»“æžœï¼š{context}
æˆ‘åº”è¯¥ï¼š1) ç½‘é¡µæœç´¢æ›´å¤šä¿¡æ¯ 2) ç”¨çŽ°æœ‰çŸ¥è¯†å›žç­”
æŒ‰yamlæ ¼å¼è¾“å‡ºï¼š
```yaml
action: search/answer
reason: é€‰æ‹©æ­¤åŠ¨ä½œçš„åŽŸå› 
search_term: å¦‚åŠ¨ä½œæ˜¯searchï¼Œå¡«å†™æœç´¢è¯
```"""
        resp = call_llm(prompt)
        yaml_str = resp.split("```yaml")[1].split("```")[0].strip()
        result = yaml.safe_load(yaml_str)
        
        assert isinstance(result, dict)
        assert "action" in result
        assert "reason" in result
        assert result["action"] in ["search", "answer"]
        if result["action"] == "search":
            assert "search_term" in result
        
        return result

    def post(self, shared, prep_res, exec_res):
        if exec_res["action"] == "search":
            shared["search_term"] = exec_res["search_term"]
        return exec_res["action"]

class SearchWeb(Node):
    def prep(self, shared):
        return shared["search_term"]
        
    def exec(self, search_term):
        return search_web(search_term)
    
    def post(self, shared, prep_res, exec_res):
        prev_searches = shared.get("context", [])
        shared["context"] = prev_searches + [
            {"term": shared["search_term"], "result": exec_res}
        ]
        return "decide"
        
class DirectAnswer(Node):
    def prep(self, shared):
        return shared["query"], shared.get("context", "")
        
    def exec(self, inputs):
        query, context = inputs
        return call_llm(f"ä¸Šä¸‹æ–‡ï¼š{context}\nå›žç­”ï¼š{query}")

    def post(self, shared, prep_res, exec_res):
       print(f"ç­”æ¡ˆï¼š{exec_res}")
       shared["answer"] = exec_res

# è¿žæŽ¥èŠ‚ç‚¹
decide = DecideAction()
search = SearchWeb()
answer = DirectAnswer()

decide - "search" >> search
decide - "answer" >> answer
search - "decide" >> decide  # å¾ªçŽ¯å›žæ¥

flow = Flow(start=decide)
flow.run({"query": "2024å¹´è¯ºè´å°”ç‰©ç†å­¦å¥–å¾—ä¸»æ˜¯è°ï¼Ÿ"})
```

================================================
File: docs/design_pattern/mapreduce.md
================================================
---
layout: default
title: "Map Reduce"
parent: "è®¾è®¡æ¨¡å¼"
nav_order: 4
---

# Map Reduce

MapReduceæ˜¯ä¸€ç§é€‚ç”¨äºŽä»¥ä¸‹æƒ…å†µçš„è®¾è®¡æ¨¡å¼ï¼š
- è¾“å…¥æ•°æ®é‡å¤§ï¼ˆå¦‚å¤šä¸ªè¦å¤„ç†çš„æ–‡ä»¶ï¼‰ï¼Œæˆ–
- è¾“å‡ºæ•°æ®é‡å¤§ï¼ˆå¦‚å¤šä¸ªè¦å¡«å†™çš„è¡¨å•ï¼‰

ä¸”æœ‰é€»è¾‘æ–¹æ³•å°†ä»»åŠ¡æ‹†åˆ†ä¸ºæ›´å°ã€ç†æƒ³æƒ…å†µä¸‹ç‹¬ç«‹çš„éƒ¨åˆ†ã€‚

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/mapreduce.png?raw=true" width="400"/>
</div>

é¦–å…ˆåœ¨æ˜ å°„é˜¶æ®µä½¿ç”¨[æ‰¹é‡èŠ‚ç‚¹](mdc:../core_abstraction/batch.md)åˆ†è§£ä»»åŠ¡ï¼Œç„¶åŽåœ¨å½’çº¦é˜¶æ®µèšåˆã€‚

### ç¤ºä¾‹ï¼šæ–‡æ¡£æ€»ç»“

```python
class SummarizeAllFiles(BatchNode):
    def prep(self, shared):
        files_dict = shared["files"]  # å¦‚10ä¸ªæ–‡ä»¶
        return list(files_dict.items())  # [("file1.txt", "aaa..."), ("file2.txt", "bbb..."), ...]

    def exec(self, one_file):
        filename, file_content = one_file
        summary_text = call_llm(f"æ€»ç»“ä»¥ä¸‹æ–‡ä»¶ï¼š\n{file_content}")
        return (filename, summary_text)

    def post(self, shared, prep_res, exec_res_list):
        shared["file_summaries"] = dict(exec_res_list)

class CombineSummaries(Node):
    def prep(self, shared):
        return shared["file_summaries"]

    def exec(self, file_summaries):
        # æ ¼å¼åŒ–ä¸ºï¼š"File1: summary\nFile2: summary...\n"
        text_list = []
        for fname, summ in file_summaries.items():
            text_list.append(f"{fname} æ€»ç»“ï¼š\n{summ}\n")
        big_text = "\n---\n".join(text_list)

        return call_llm(f"å°†è¿™äº›æ–‡ä»¶æ€»ç»“åˆå¹¶ä¸ºä¸€ä¸ªæœ€ç»ˆæ€»ç»“ï¼š\n{big_text}")

    def post(self, shared, prep_res, final_summary):
        shared["all_files_summary"] = final_summary

batch_node = SummarizeAllFiles()
combine_node = CombineSummaries()
batch_node >> combine_node

flow = Flow(start=batch_node)

shared = {
    "files": {
        "file1.txt": "çˆ±ä¸½ä¸å¼€å§‹åŽŒå€¦äº†ååœ¨å§å§èº«è¾¹...",
        "file2.txt": "å…¶ä»–æœ‰è¶£çš„æ–‡æœ¬...",
        # ...
    }
}
flow.run(shared)
print("å„æ–‡ä»¶æ€»ç»“ï¼š", shared["file_summaries"])
print("\næœ€ç»ˆæ€»ç»“ï¼š\n", shared["all_files_summary"])
```

================================================
File: docs/design_pattern/rag.md
================================================
---
layout: default
title: "RAG"
parent: "è®¾è®¡æ¨¡å¼"
nav_order: 3
---

# RAGï¼ˆæ£€ç´¢å¢žå¼ºç”Ÿæˆï¼‰

å¯¹äºŽæŸäº›å¤§è¯­è¨€æ¨¡åž‹ä»»åŠ¡ï¼ˆå¦‚é—®ç­”ï¼‰ï¼Œæä¾›ç›¸å…³ä¸Šä¸‹æ–‡è‡³å…³é‡è¦ã€‚ä¸€ç§å¸¸è§æž¶æž„æ˜¯**ä¸¤é˜¶æ®µ**RAGæµæ°´çº¿ï¼š

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/rag.png?raw=true" width="400"/>
</div>

1. **ç¦»çº¿é˜¶æ®µ**ï¼šé¢„å¤„ç†å¹¶ç´¢å¼•æ–‡æ¡£ï¼ˆâ€œæž„å»ºç´¢å¼•â€ï¼‰ã€‚
2. **åœ¨çº¿é˜¶æ®µ**ï¼šç»™å®šé—®é¢˜ï¼Œé€šè¿‡æ£€ç´¢æœ€ç›¸å…³çš„ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆã€‚

---
## é˜¶æ®µ1ï¼šç¦»çº¿ç´¢å¼•

æˆ‘ä»¬åˆ›å»ºä¸‰ä¸ªèŠ‚ç‚¹ï¼š
1. `ChunkDocs` â€“ [åˆ†å—](mdc:../utility_function/chunking.md)åŽŸå§‹æ–‡æœ¬ã€‚
2. `EmbedDocs` â€“ [åµŒå…¥](mdc:../utility_function/embedding.md)æ¯ä¸ªå—ã€‚
3. `StoreIndex` â€“ å°†åµŒå…¥å­˜å‚¨åˆ°[å‘é‡æ•°æ®åº“](mdc:../utility_function/vector.md)ã€‚

```python
class ChunkDocs(BatchNode):
    def prep(self, shared):
        # shared["files"]ä¸­çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ã€‚æˆ‘ä»¬å¤„ç†æ¯ä¸ªæ–‡ä»¶ã€‚
        return shared["files"]

    def exec(self, filepath):
        # è¯»å–æ–‡ä»¶å†…å®¹ã€‚å®žé™…ä½¿ç”¨ä¸­éœ€å¤„ç†é”™è¯¯ã€‚
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        # æ¯100ä¸ªå­—ç¬¦åˆ†ä¸€å—
        chunks = []
        size = 100
        for i in range(0, len(text), size):
            chunks.append(text[i : i + size])
        return chunks
    
    def post(self, shared, prep_res, exec_res_list):
        # exec_res_listæ˜¯æ¯ä¸ªæ–‡ä»¶çš„å—åˆ—è¡¨ã€‚
        # å°†å®ƒä»¬å…¨éƒ¨å±•å¹³ä¸ºå•ä¸ªå—åˆ—è¡¨ã€‚
        all_chunks = []
        for chunk_list in exec_res_list:
            all_chunks.extend(chunk_list)
        shared["all_chunks"] = all_chunks

class EmbedDocs(BatchNode):
    def prep(self, shared):
        return shared["all_chunks"]

    def exec(self, chunk):
        return get_embedding(chunk)

    def post(self, shared, prep_res, exec_res_list):
        # å­˜å‚¨åµŒå…¥åˆ—è¡¨ã€‚
        shared["all_embeds"] = exec_res_list
        print(f"æ€»åµŒå…¥æ•°ï¼š{len(exec_res_list)}")

class StoreIndex(Node):
    def prep(self, shared):
        # ä»Žå…±äº«å­˜å‚¨è¯»å–æ‰€æœ‰åµŒå…¥ã€‚
        return shared["all_embeds"]

    def exec(self, all_embeds):
        # åˆ›å»ºå‘é‡ç´¢å¼•ï¼ˆå®žé™…ä½¿ç”¨ä¸­ç”¨faissæˆ–å…¶ä»–æ•°æ®åº“ï¼‰ã€‚
        index = create_index(all_embeds)
        return index

    def post(self, shared, prep_res, index):
        shared["index"] = index

# æŒ‰é¡ºåºè¿žæŽ¥
chunk_node = ChunkDocs()
embed_node = EmbedDocs()
store_node = StoreIndex()

chunk_node >> embed_node >> store_node

OfflineFlow = Flow(start=chunk_node)
```

ä½¿ç”¨ç¤ºä¾‹ï¼š

```python
shared = {
    "files": ["doc1.txt", "doc2.txt"],  # ä»»ä½•æ–‡æœ¬æ–‡ä»¶
}
OfflineFlow.run(shared)
```

---
## é˜¶æ®µ2ï¼šåœ¨çº¿æŸ¥è¯¢ä¸Žå›žç­”

æˆ‘ä»¬æœ‰3ä¸ªèŠ‚ç‚¹ï¼š
1. `EmbedQuery` â€“ åµŒå…¥ç”¨æˆ·çš„é—®é¢˜ã€‚
2. `RetrieveDocs` â€“ ä»Žç´¢å¼•æ£€ç´¢é¡¶çº§å—ã€‚
3. `GenerateAnswer` â€“ è°ƒç”¨å¤§è¯­è¨€æ¨¡åž‹ï¼Œç»“åˆé—®é¢˜å’Œå—ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆã€‚

```python
class EmbedQuery(Node):
    def prep(self, shared):
        return shared["question"]

    def exec(self, question):
        return get_embedding(question)

    def post(self, shared, prep_res, q_emb):
        shared["q_emb"] = q_emb

class RetrieveDocs(Node):
    def prep(self, shared):
        # éœ€è¦æŸ¥è¯¢åµŒå…¥ã€ç¦»çº¿ç´¢å¼•å’Œå—
        return shared["q_emb"], shared["index"], shared["all_chunks"]

    def exec(self, inputs):
        q_emb, index, chunks = inputs
        I, D = search_index(index, q_emb, top_k=1)
        best_id = I[0][0]
        relevant_chunk = chunks[best_id]
        return relevant_chunk

    def post(self, shared, prep_res, relevant_chunk):
        shared["retrieved_chunk"] = relevant_chunk
        print("æ£€ç´¢åˆ°çš„å—ï¼š", relevant_chunk[:60], "...")

class GenerateAnswer(Node):
    def prep(self, shared):
        return shared["question"], shared["retrieved_chunk"]

    def exec(self, inputs):
        question, chunk = inputs
        prompt = f"é—®é¢˜ï¼š{question}\nä¸Šä¸‹æ–‡ï¼š{chunk}\nç­”æ¡ˆï¼š"
        return call_llm(prompt)

    def post(self, shared, prep_res, answer):
        shared["answer"] = answer
        print("ç­”æ¡ˆï¼š", answer)

embed_qnode = EmbedQuery()
retrieve_node = RetrieveDocs()
generate_node = GenerateAnswer()

embed_qnode >> retrieve_node >> generate_node
OnlineFlow = Flow(start=embed_qnode)
```

ä½¿ç”¨ç¤ºä¾‹ï¼š

```python
# å‡è®¾å·²è¿è¡ŒOfflineFlowï¼Œæœ‰ï¼š
# shared["all_chunks"], shared["index"], ç­‰ã€‚
shared["question"] = "äººä»¬ä¸ºä»€ä¹ˆå–œæ¬¢çŒ«ï¼Ÿ"

OnlineFlow.run(shared)
# æœ€ç»ˆç­”æ¡ˆåœ¨shared["answer"]ä¸­
```

================================================
File: docs/design_pattern/structure.md
================================================
---
layout: default
title: "ç»“æž„åŒ–è¾“å‡º"
parent: "è®¾è®¡æ¨¡å¼"
nav_order: 5
---

# ç»“æž„åŒ–è¾“å‡º

åœ¨è®¸å¤šç”¨ä¾‹ä¸­ï¼Œä½ å¯èƒ½å¸Œæœ›å¤§è¯­è¨€æ¨¡åž‹è¾“å‡ºç‰¹å®šç»“æž„ï¼Œå¦‚åˆ—è¡¨æˆ–å…·æœ‰é¢„å®šä¹‰é”®çš„å­—å…¸ã€‚

æœ‰å‡ ç§æ–¹æ³•å¯å®žçŽ°ç»“æž„åŒ–è¾“å‡ºï¼š
- **æç¤º**å¤§è¯­è¨€æ¨¡åž‹ä¸¥æ ¼è¿”å›žå®šä¹‰çš„ç»“æž„ã€‚
- ä½¿ç”¨åŽŸç”Ÿæ”¯æŒ** schema å¼ºåˆ¶**çš„å¤§è¯­è¨€æ¨¡åž‹ã€‚
- **åŽå¤„ç†**å¤§è¯­è¨€æ¨¡åž‹çš„å“åº”ä»¥æå–ç»“æž„åŒ–å†…å®¹ã€‚

å®žé™…ä¸Šï¼Œ**æç¤º**å¯¹çŽ°ä»£å¤§è¯­è¨€æ¨¡åž‹ç®€å•ä¸”å¯é ã€‚

### ç¤ºä¾‹ç”¨ä¾‹

- æå–å…³é”®ä¿¡æ¯

```yaml
product:
  name: Widget Pro
  price: 199.99
  description: |
    ä¸ºä¸“ä¸šäººå£«è®¾è®¡çš„é«˜è´¨é‡éƒ¨ä»¶ã€‚
    æŽ¨èç»™é«˜çº§ç”¨æˆ·ã€‚
```

- å°†æ–‡æ¡£æ€»ç»“ä¸ºè¦ç‚¹

```yaml
summary:
  - æœ¬äº§å“æ˜“äºŽä½¿ç”¨ã€‚
  - æ€§ä»·æ¯”é«˜ã€‚
  - é€‚åˆæ‰€æœ‰æŠ€èƒ½æ°´å¹³ã€‚
```

- ç”Ÿæˆé…ç½®æ–‡ä»¶

```yaml
server:
  host: 127.0.0.1
  port: 8080
  ssl: true
```

## æç¤ºå·¥ç¨‹

æç¤ºå¤§è¯­è¨€æ¨¡åž‹ç”Ÿæˆ**ç»“æž„åŒ–**è¾“å‡ºæ—¶ï¼š
1. ç”¨ä»£ç å›´æ **åŒ…è£¹**ç»“æž„ï¼ˆå¦‚`yaml`ï¼‰ã€‚
2. **éªŒè¯**æ‰€æœ‰å¿…å¡«å­—æ®µå­˜åœ¨ï¼ˆå¹¶è®©`Node`å¤„ç†é‡è¯•ï¼‰ã€‚

### ç¤ºä¾‹æ–‡æœ¬æ€»ç»“

```python
class SummarizeNode(Node):
    def exec(self, prep_res):
        # å‡è®¾`prep_res`æ˜¯è¦æ€»ç»“çš„æ–‡æœ¬ã€‚
        prompt = f"""
è¯·å°†ä»¥ä¸‹æ–‡æœ¬æ€»ç»“ä¸ºYAMLï¼Œ exactly 3ä¸ªè¦ç‚¹

{prep_res}

çŽ°åœ¨ï¼Œè¾“å‡ºï¼š
```yaml
summary:
  - è¦ç‚¹1
  - è¦ç‚¹2
  - è¦ç‚¹3
```"""
        response = call_llm(prompt)
        yaml_str = response.split("```yaml")[1].split("```")[0].strip()

        import yaml
        structured_result = yaml.safe_load(yaml_str)

        assert "summary" in structured_result
        assert isinstance(structured_result["summary"], list)

        return structured_result
```

> é™¤äº†ä½¿ç”¨`assert`è¯­å¥ï¼Œå¦ä¸€ç§æµè¡Œçš„ schema éªŒè¯æ–¹å¼æ˜¯[Pydantic](mdc:https:/github.com/pydantic/pydantic)
{: .note }

### ä¸ºä»€ä¹ˆç”¨YAMLè€Œä¸æ˜¯JSONï¼Ÿ

å½“å‰å¤§è¯­è¨€æ¨¡åž‹åœ¨è½¬ä¹‰æ–¹é¢æœ‰å›°éš¾ã€‚YAMLå¤„ç†å­—ç¬¦ä¸²æ›´ç®€å•ï¼Œå› ä¸ºå®ƒä»¬ä¸æ€»æ˜¯éœ€è¦å¼•å·ã€‚

**JSONä¸­**  

```json
{
  "dialogue": "Alice said: \"Hello Bob.\\nHow are you?\\nI am good.\""
}
```

- å­—ç¬¦ä¸²å†…çš„æ¯ä¸ªåŒå¼•å·å¿…é¡»ç”¨`\"`è½¬ä¹‰ã€‚
- å¯¹è¯ä¸­çš„æ¯ä¸ªæ¢è¡Œå¿…é¡»è¡¨ç¤ºä¸º`\n`ã€‚

**YAMLä¸­**  

```yaml
dialogue: |
  Alice said: "Hello Bob.
  How are you?
  I am good."
```

- æ— éœ€è½¬ä¹‰å†…éƒ¨å¼•å·â€”â€”åªéœ€å°†æ•´ä¸ªæ–‡æœ¬æ”¾åœ¨å—å­—é¢é‡ï¼ˆ`|`ï¼‰ä¸‹ã€‚
- æ¢è¡Œè‡ªç„¶ä¿ç•™ï¼Œæ— éœ€`\n`ã€‚

================================================
File: docs/design_pattern/workflow.md
================================================
---
layout: default
title: "å·¥ä½œæµ"
parent: "è®¾è®¡æ¨¡å¼"
nav_order: 2
---

# å·¥ä½œæµ

è®¸å¤šçŽ°å®žä¸–ç•Œçš„ä»»åŠ¡å¤ªå¤æ‚ï¼Œæ— æ³•é€šè¿‡ä¸€æ¬¡å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨å®Œæˆã€‚è§£å†³æ–¹æ¡ˆæ˜¯**ä»»åŠ¡åˆ†è§£**ï¼šå°†å®ƒä»¬åˆ†è§£ä¸º[é“¾å¼](mdc:../core_abstraction/flow.md)çš„å¤šä¸ªèŠ‚ç‚¹ã€‚

<div align="center">
  <img src="https://github.com/the-pocket/.github/raw/main/assets/workflow.png?raw=true" width="400"/>
</div>

> - ä¸è¦è®©æ¯ä¸ªä»»åŠ¡**å¤ªç²—ç³™**ï¼Œå› ä¸ºå®ƒå¯èƒ½*å¤ªå¤æ‚ï¼Œæ— æ³•é€šè¿‡ä¸€æ¬¡å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨å®Œæˆ*ã€‚
> - ä¸è¦è®©æ¯ä¸ªä»»åŠ¡**å¤ªç²¾ç»†**ï¼Œå› ä¸ºè¿™æ ·*å¤§è¯­è¨€æ¨¡åž‹è°ƒç”¨æ²¡æœ‰è¶³å¤Ÿçš„ä¸Šä¸‹æ–‡*ï¼Œä¸”ç»“æžœåœ¨èŠ‚ç‚¹é—´*ä¸ä¸€è‡´*ã€‚
> 
> é€šå¸¸éœ€è¦å¤šæ¬¡*è¿­ä»£*æ‰èƒ½æ‰¾åˆ°*æœ€ä½³å¹³è¡¡ç‚¹*ã€‚å¦‚æžœä»»åŠ¡æœ‰å¤ªå¤š*è¾¹ç¼˜æƒ…å†µ*ï¼Œè€ƒè™‘ä½¿ç”¨[æ™ºèƒ½ä½“](mdc:agent.md)ã€‚
{: .best-practice }

### ç¤ºä¾‹ï¼šæ–‡ç« å†™ä½œ

```python
class GenerateOutline(Node):
    def prep(self, shared): return shared["topic"]
    def exec(self, topic): return call_llm(f"ä¸ºå…³äºŽ{topic}çš„æ–‡ç« åˆ›å»ºè¯¦ç»†å¤§çº²")
    def post(self, shared, prep_res, exec_res): shared["outline"] = exec_res

class WriteSection(Node):
    def prep(self, shared): return shared["outline"]
    def exec(self, outline): return call_llm(f"åŸºäºŽæ­¤å¤§çº²æ’°å†™å†…å®¹ï¼š{outline}")
    def post(self, shared, prep_res, exec_res): shared["draft"] = exec_res

class ReviewAndRefine(Node):
    def prep(self, shared): return shared["draft"]
    def exec(self, draft): return call_llm(f"å®¡æŸ¥å¹¶æ”¹è¿›æ­¤è‰ç¨¿ï¼š{draft}")
    def post(self, shared, prep_res, exec_res): shared["final_article"] = exec_res

# è¿žæŽ¥èŠ‚ç‚¹
outline = GenerateOutline()
write = WriteSection()
review = ReviewAndRefine()

outline >> write >> review

# åˆ›å»ºå¹¶è¿è¡Œæµç¨‹
writing_flow = Flow(start=outline)
shared = {"topic": "AIå®‰å…¨"}
writing_flow.run(shared)
```

å¯¹äºŽ*åŠ¨æ€æƒ…å†µ*ï¼Œè€ƒè™‘ä½¿ç”¨[æ™ºèƒ½ä½“](mdc:agent.md)ã€‚

================================================
File: docs/utility_function/llm.md
================================================
---
layout: default
title: "å¤§è¯­è¨€æ¨¡åž‹å°è£…"
parent: "å·¥å…·å‡½æ•°"
nav_order: 1
---

# å¤§è¯­è¨€æ¨¡åž‹å°è£…

æŸ¥çœ‹[litellm](mdc:https:/github.com/BerriAI/litellm)ç­‰åº“ã€‚
è¿™é‡Œæä¾›ä¸€äº›æœ€å°åŒ–çš„ç¤ºä¾‹å®žçŽ°ï¼š

1. OpenAI
    ```python
    def call_llm(prompt):
        from openai import OpenAI
        client = OpenAI(api_key="YOUR_API_KEY_HERE")
        r = client.chat.completions.create(
            model="gpt-4o",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content

    # ç¤ºä¾‹ç”¨æ³•
    call_llm("ä½ å¥½å—ï¼Ÿ")
    ```
    > å‡ºäºŽå®‰å…¨è€ƒè™‘ï¼Œå°†APIå¯†é’¥å­˜å‚¨åœ¨çŽ¯å¢ƒå˜é‡ä¸­ï¼Œå¦‚OPENAI_API_KEYã€‚
    {: .best-practice }

2. Claudeï¼ˆAnthropicï¼‰
    ```python
    def call_llm(prompt):
        from anthropic import Anthropic
        client = Anthropic(api_key="YOUR_API_KEY_HERE")
        response = client.messages.create(
            model="claude-2",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=100
        )
        return response.content
    ```

3. Googleï¼ˆGenerative AI Studio / PaLM APIï¼‰
    ```python
    def call_llm(prompt):
        import google.generativeai as genai
        genai.configure(api_key="YOUR_API_KEY_HERE")
        response = genai.generate_text(
            model="models/text-bison-001",
            prompt=prompt
        )
        return response.result
    ```

4. Azureï¼ˆAzure OpenAIï¼‰
    ```python
    def call_llm(prompt):
        from openai import AzureOpenAI
        client = AzureOpenAI(
            azure_endpoint="https://<YOUR_RESOURCE_NAME>.openai.azure.com/",
            api_key="YOUR_API_KEY_HERE",
            api_version="2023-05-15"
        )
        r = client.chat.completions.create(
            model="<YOUR_DEPLOYMENT_NAME>",
            messages=[{"role": "user", "content": prompt}]
        )
        return r.choices[0].message.content
    ```

5. Ollamaï¼ˆæœ¬åœ°å¤§è¯­è¨€æ¨¡åž‹ï¼‰
    ```python
    def call_llm(prompt):
        from ollama import chat
        response = chat(
            model="llama2",
            messages=[{"role": "user", "content": prompt}]
        )
        return response.message.content
    ```

## æ”¹è¿›

æ ¹æ®éœ€è¦å¢žå¼º`call_llm`å‡½æ•°ã€‚ç¤ºä¾‹ï¼š

- å¤„ç†èŠå¤©åŽ†å²ï¼š

```python
def call_llm(messages):
    from openai import OpenAI
    client = OpenAI(api_key="YOUR_API_KEY_HERE")
    r = client.chat.completions.create(
        model="gpt-4o",
        messages=messages
    )
    return r.choices[0].message.content
```

- æ·»åŠ å†…å­˜ç¼“å­˜

```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def call_llm(prompt):
    # ä½ çš„å®žçŽ°
    pass
```

> âš ï¸ ç¼“å­˜ä¸ŽèŠ‚ç‚¹é‡è¯•å†²çªï¼Œå› ä¸ºé‡è¯•ä¼šäº§ç”Ÿç›¸åŒç»“æžœã€‚
>
> ä¸ºè§£å†³æ­¤é—®é¢˜ï¼Œå¯ä»…åœ¨æœªé‡è¯•æ—¶ä½¿ç”¨ç¼“å­˜ç»“æžœã€‚
{: .warning }


```python
from functools import lru_cache

@lru_cache(maxsize=1000)
def cached_call(prompt):
    pass

def call_llm(prompt, use_cache):
    if use_cache:
        return cached_call(prompt)
    # ç›´æŽ¥è°ƒç”¨åº•å±‚å‡½æ•°
    return cached_call.__wrapped__(prompt)

class SummarizeNode(Node):
    def exec(self, text):
        return call_llm(f"æ€»ç»“ï¼š{text}", self.cur_retry==0)
```

- å¯ç”¨æ—¥å¿—ï¼š

```python
def call_llm(prompt):
    import logging
    logging.info(f"æç¤ºï¼š{prompt}")
    response = ... # ä½ çš„å®žçŽ°
    logging.info(f"å“åº”ï¼š{response}")
    return response
```


